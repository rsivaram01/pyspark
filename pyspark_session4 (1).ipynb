{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a8d827-01aa-4868-a37c-26495709a650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/Volumes/sample_cat/default/volume_csv/emp_dataset/departments.csv\n",
    "\n",
    "/Volumes/sample_cat/default/volume_csv/emp_dataset/employees.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e467bd69-9316-4d39-b85b-7bd02a473730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df = spark.read.option(\"header\",True)\\\n",
    "                    .option(\"inferSchema\",True)\\\n",
    "                    .csv('/Volumes/sample_cat/default/volume_csv/emp_dataset/employees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c4f07e-9053-496d-a8b7-b468bacd1272",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"EMAIL\":157},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761053909367}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EMPLOYEE_ID</th><th>FIRST_NAME</th><th>LAST_NAME</th><th>EMAIL</th><th>PHONE_NUMBER</th><th>HIRE_DATE</th><th>JOB_ID</th><th>SALARY</th><th>COMMISSION_PCT</th><th>MANAGER_ID</th><th>DEPARTMENT_ID</th></tr></thead><tbody><tr><td>198</td><td>Donald</td><td>OConnell</td><td>DOCONNEL</td><td>650.507.9833</td><td>21-JUN-07</td><td>SH_CLERK</td><td>2600</td><td> - </td><td>124</td><td>50</td></tr><tr><td>199</td><td>Douglas</td><td>Grant</td><td>DGRANT</td><td>650.507.9844</td><td>13-JAN-08</td><td>SH_CLERK</td><td>2600</td><td> - </td><td>124</td><td>50</td></tr><tr><td>200</td><td>Jennifer</td><td>Whalen</td><td>JWHALEN</td><td>515.123.4444</td><td>17-SEP-03</td><td>AD_ASST</td><td>4400</td><td> - </td><td>101</td><td>10</td></tr><tr><td>201</td><td>Michael</td><td>Hartstein</td><td>MHARTSTE</td><td>515.123.5555</td><td>17-FEB-04</td><td>MK_MAN</td><td>13000</td><td> - </td><td>100</td><td>20</td></tr><tr><td>202</td><td>Pat</td><td>Fay</td><td>PFAY</td><td>603.123.6666</td><td>17-AUG-05</td><td>MK_REP</td><td>6000</td><td> - </td><td>201</td><td>20</td></tr><tr><td>203</td><td>Susan</td><td>Mavris</td><td>SMAVRIS</td><td>515.123.7777</td><td>07-JUN-02</td><td>HR_REP</td><td>6500</td><td> - </td><td>101</td><td>40</td></tr><tr><td>204</td><td>Hermann</td><td>Baer</td><td>HBAER</td><td>515.123.8888</td><td>07-JUN-02</td><td>PR_REP</td><td>10000</td><td> - </td><td>101</td><td>70</td></tr><tr><td>205</td><td>Shelley</td><td>Higgins</td><td>SHIGGINS</td><td>515.123.8080</td><td>07-JUN-02</td><td>AC_MGR</td><td>12008</td><td> - </td><td>101</td><td>110</td></tr><tr><td>206</td><td>William</td><td>Gietz</td><td>WGIETZ</td><td>515.123.8181</td><td>07-JUN-02</td><td>AC_ACCOUNT</td><td>8300</td><td> - </td><td>205</td><td>110</td></tr><tr><td>100</td><td>Steven</td><td>King</td><td>SKING</td><td>515.123.4567</td><td>17-JUN-03</td><td>AD_PRES</td><td>24000</td><td> - </td><td> - </td><td>90</td></tr><tr><td>101</td><td>Neena</td><td>Kochhar</td><td>NKOCHHAR</td><td>515.123.4568</td><td>21-SEP-05</td><td>AD_VP</td><td>17000</td><td> - </td><td>100</td><td>90</td></tr><tr><td>102</td><td>Lex</td><td>De Haan</td><td>LDEHAAN</td><td>515.123.4569</td><td>13-JAN-01</td><td>AD_VP</td><td>17000</td><td> - </td><td>100</td><td>90</td></tr><tr><td>103</td><td>Alexander</td><td>Hunold</td><td>AHUNOLD</td><td>590.423.4567</td><td>03-JAN-06</td><td>IT_PROG</td><td>9000</td><td> - </td><td>102</td><td>60</td></tr><tr><td>104</td><td>Bruce</td><td>Ernst</td><td>BERNST</td><td>590.423.4568</td><td>21-MAY-07</td><td>IT_PROG</td><td>6000</td><td> - </td><td>103</td><td>60</td></tr><tr><td>105</td><td>David</td><td>Austin</td><td>DAUSTIN</td><td>590.423.4569</td><td>25-JUN-05</td><td>IT_PROG</td><td>4800</td><td> - </td><td>103</td><td>60</td></tr><tr><td>106</td><td>Valli</td><td>Pataballa</td><td>VPATABAL</td><td>590.423.4560</td><td>05-FEB-06</td><td>IT_PROG</td><td>4800</td><td> - </td><td>103</td><td>60</td></tr><tr><td>107</td><td>Diana</td><td>Lorentz</td><td>DLORENTZ</td><td>590.423.5567</td><td>07-FEB-07</td><td>IT_PROG</td><td>4200</td><td> - </td><td>103</td><td>60</td></tr><tr><td>108</td><td>Nancy</td><td>Greenberg</td><td>NGREENBE</td><td>515.124.4569</td><td>17-AUG-02</td><td>FI_MGR</td><td>12008</td><td> - </td><td>101</td><td>100</td></tr><tr><td>109</td><td>Daniel</td><td>Faviet</td><td>DFAVIET</td><td>515.124.4169</td><td>16-AUG-02</td><td>FI_ACCOUNT</td><td>9000</td><td> - </td><td>108</td><td>100</td></tr><tr><td>110</td><td>John</td><td>Chen</td><td>JCHEN</td><td>515.124.4269</td><td>28-SEP-05</td><td>FI_ACCOUNT</td><td>8200</td><td> - </td><td>108</td><td>100</td></tr><tr><td>111</td><td>Ismael</td><td>Sciarra</td><td>ISCIARRA</td><td>515.124.4369</td><td>30-SEP-05</td><td>FI_ACCOUNT</td><td>7700</td><td> - </td><td>108</td><td>100</td></tr><tr><td>112</td><td>Jose Manuel</td><td>Urman</td><td>JMURMAN</td><td>515.124.4469</td><td>07-MAR-06</td><td>FI_ACCOUNT</td><td>7800</td><td> - </td><td>108</td><td>100</td></tr><tr><td>113</td><td>Luis</td><td>Popp</td><td>LPOPP</td><td>515.124.4567</td><td>07-DEC-07</td><td>FI_ACCOUNT</td><td>6900</td><td> - </td><td>108</td><td>100</td></tr><tr><td>114</td><td>Den</td><td>Raphaely</td><td>DRAPHEAL</td><td>515.127.4561</td><td>07-DEC-02</td><td>PU_MAN</td><td>11000</td><td> - </td><td>100</td><td>30</td></tr><tr><td>115</td><td>Alexander</td><td>Khoo</td><td>AKHOO</td><td>515.127.4562</td><td>18-MAY-03</td><td>PU_CLERK</td><td>3100</td><td> - </td><td>114</td><td>30</td></tr><tr><td>116</td><td>Shelli</td><td>Baida</td><td>SBAIDA</td><td>515.127.4563</td><td>24-DEC-05</td><td>PU_CLERK</td><td>2900</td><td> - </td><td>114</td><td>30</td></tr><tr><td>117</td><td>Sigal</td><td>Tobias</td><td>STOBIAS</td><td>515.127.4564</td><td>24-JUL-05</td><td>PU_CLERK</td><td>2800</td><td> - </td><td>114</td><td>30</td></tr><tr><td>118</td><td>Guy</td><td>Himuro</td><td>GHIMURO</td><td>515.127.4565</td><td>15-NOV-06</td><td>PU_CLERK</td><td>2600</td><td> - </td><td>114</td><td>30</td></tr><tr><td>119</td><td>Karen</td><td>Colmenares</td><td>KCOLMENA</td><td>515.127.4566</td><td>10-AUG-07</td><td>PU_CLERK</td><td>2500</td><td> - </td><td>114</td><td>30</td></tr><tr><td>120</td><td>Matthew</td><td>Weiss</td><td>MWEISS</td><td>650.123.1234</td><td>18-JUL-04</td><td>ST_MAN</td><td>8000</td><td> - </td><td>100</td><td>50</td></tr><tr><td>121</td><td>Adam</td><td>Fripp</td><td>AFRIPP</td><td>650.123.2234</td><td>10-APR-05</td><td>ST_MAN</td><td>8200</td><td> - </td><td>100</td><td>50</td></tr><tr><td>122</td><td>Payam</td><td>Kaufling</td><td>PKAUFLIN</td><td>650.123.3234</td><td>01-MAY-03</td><td>ST_MAN</td><td>7900</td><td> - </td><td>100</td><td>50</td></tr><tr><td>123</td><td>Shanta</td><td>Vollman</td><td>SVOLLMAN</td><td>650.123.4234</td><td>10-OCT-05</td><td>ST_MAN</td><td>6500</td><td> - </td><td>100</td><td>50</td></tr><tr><td>124</td><td>Kevin</td><td>Mourgos</td><td>KMOURGOS</td><td>650.123.5234</td><td>16-NOV-07</td><td>ST_MAN</td><td>5800</td><td> - </td><td>100</td><td>50</td></tr><tr><td>125</td><td>Julia</td><td>Nayer</td><td>JNAYER</td><td>650.124.1214</td><td>16-JUL-05</td><td>ST_CLERK</td><td>3200</td><td> - </td><td>120</td><td>50</td></tr><tr><td>126</td><td>Irene</td><td>Mikkilineni</td><td>IMIKKILI</td><td>650.124.1224</td><td>28-SEP-06</td><td>ST_CLERK</td><td>2700</td><td> - </td><td>120</td><td>50</td></tr><tr><td>127</td><td>James</td><td>Landry</td><td>JLANDRY</td><td>650.124.1334</td><td>14-JAN-07</td><td>ST_CLERK</td><td>2400</td><td> - </td><td>120</td><td>50</td></tr><tr><td>128</td><td>Steven</td><td>Markle</td><td>SMARKLE</td><td>650.124.1434</td><td>08-MAR-08</td><td>ST_CLERK</td><td>2200</td><td> - </td><td>120</td><td>50</td></tr><tr><td>129</td><td>Laura</td><td>Bissot</td><td>LBISSOT</td><td>650.124.5234</td><td>20-AUG-05</td><td>ST_CLERK</td><td>3300</td><td> - </td><td>121</td><td>50</td></tr><tr><td>130</td><td>Mozhe</td><td>Atkinson</td><td>MATKINSO</td><td>650.124.6234</td><td>30-OCT-05</td><td>ST_CLERK</td><td>2800</td><td> - </td><td>121</td><td>50</td></tr><tr><td>131</td><td>James</td><td>Marlow</td><td>JAMRLOW</td><td>650.124.7234</td><td>16-FEB-05</td><td>ST_CLERK</td><td>2500</td><td> - </td><td>121</td><td>50</td></tr><tr><td>132</td><td>TJ</td><td>Olson</td><td>TJOLSON</td><td>650.124.8234</td><td>10-APR-07</td><td>ST_CLERK</td><td>2100</td><td> - </td><td>121</td><td>50</td></tr><tr><td>133</td><td>Jason</td><td>Mallin</td><td>JMALLIN</td><td>650.127.1934</td><td>14-JUN-04</td><td>ST_CLERK</td><td>3300</td><td> - </td><td>122</td><td>50</td></tr><tr><td>134</td><td>Michael</td><td>Rogers</td><td>MROGERS</td><td>650.127.1834</td><td>26-AUG-06</td><td>ST_CLERK</td><td>2900</td><td> - </td><td>122</td><td>50</td></tr><tr><td>135</td><td>Ki</td><td>Gee</td><td>KGEE</td><td>650.127.1734</td><td>12-DEC-07</td><td>ST_CLERK</td><td>2400</td><td> - </td><td>122</td><td>50</td></tr><tr><td>136</td><td>Hazel</td><td>Philtanker</td><td>HPHILTAN</td><td>650.127.1634</td><td>06-FEB-08</td><td>ST_CLERK</td><td>2200</td><td> - </td><td>122</td><td>50</td></tr><tr><td>137</td><td>Renske</td><td>Ladwig</td><td>RLADWIG</td><td>650.121.1234</td><td>14-JUL-03</td><td>ST_CLERK</td><td>3600</td><td> - </td><td>123</td><td>50</td></tr><tr><td>138</td><td>Stephen</td><td>Stiles</td><td>SSTILES</td><td>650.121.2034</td><td>26-OCT-05</td><td>ST_CLERK</td><td>3200</td><td> - </td><td>123</td><td>50</td></tr><tr><td>139</td><td>John</td><td>Seo</td><td>JSEO</td><td>650.121.2019</td><td>12-FEB-06</td><td>ST_CLERK</td><td>2700</td><td> - </td><td>123</td><td>50</td></tr><tr><td>140</td><td>Joshua</td><td>Patel</td><td>JPATEL</td><td>650.121.1834</td><td>06-APR-06</td><td>ST_CLERK</td><td>2500</td><td> - </td><td>123</td><td>50</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         198,
         "Donald",
         "OConnell",
         "DOCONNEL",
         "650.507.9833",
         "21-JUN-07",
         "SH_CLERK",
         2600,
         " - ",
         "124",
         50
        ],
        [
         199,
         "Douglas",
         "Grant",
         "DGRANT",
         "650.507.9844",
         "13-JAN-08",
         "SH_CLERK",
         2600,
         " - ",
         "124",
         50
        ],
        [
         200,
         "Jennifer",
         "Whalen",
         "JWHALEN",
         "515.123.4444",
         "17-SEP-03",
         "AD_ASST",
         4400,
         " - ",
         "101",
         10
        ],
        [
         201,
         "Michael",
         "Hartstein",
         "MHARTSTE",
         "515.123.5555",
         "17-FEB-04",
         "MK_MAN",
         13000,
         " - ",
         "100",
         20
        ],
        [
         202,
         "Pat",
         "Fay",
         "PFAY",
         "603.123.6666",
         "17-AUG-05",
         "MK_REP",
         6000,
         " - ",
         "201",
         20
        ],
        [
         203,
         "Susan",
         "Mavris",
         "SMAVRIS",
         "515.123.7777",
         "07-JUN-02",
         "HR_REP",
         6500,
         " - ",
         "101",
         40
        ],
        [
         204,
         "Hermann",
         "Baer",
         "HBAER",
         "515.123.8888",
         "07-JUN-02",
         "PR_REP",
         10000,
         " - ",
         "101",
         70
        ],
        [
         205,
         "Shelley",
         "Higgins",
         "SHIGGINS",
         "515.123.8080",
         "07-JUN-02",
         "AC_MGR",
         12008,
         " - ",
         "101",
         110
        ],
        [
         206,
         "William",
         "Gietz",
         "WGIETZ",
         "515.123.8181",
         "07-JUN-02",
         "AC_ACCOUNT",
         8300,
         " - ",
         "205",
         110
        ],
        [
         100,
         "Steven",
         "King",
         "SKING",
         "515.123.4567",
         "17-JUN-03",
         "AD_PRES",
         24000,
         " - ",
         " - ",
         90
        ],
        [
         101,
         "Neena",
         "Kochhar",
         "NKOCHHAR",
         "515.123.4568",
         "21-SEP-05",
         "AD_VP",
         17000,
         " - ",
         "100",
         90
        ],
        [
         102,
         "Lex",
         "De Haan",
         "LDEHAAN",
         "515.123.4569",
         "13-JAN-01",
         "AD_VP",
         17000,
         " - ",
         "100",
         90
        ],
        [
         103,
         "Alexander",
         "Hunold",
         "AHUNOLD",
         "590.423.4567",
         "03-JAN-06",
         "IT_PROG",
         9000,
         " - ",
         "102",
         60
        ],
        [
         104,
         "Bruce",
         "Ernst",
         "BERNST",
         "590.423.4568",
         "21-MAY-07",
         "IT_PROG",
         6000,
         " - ",
         "103",
         60
        ],
        [
         105,
         "David",
         "Austin",
         "DAUSTIN",
         "590.423.4569",
         "25-JUN-05",
         "IT_PROG",
         4800,
         " - ",
         "103",
         60
        ],
        [
         106,
         "Valli",
         "Pataballa",
         "VPATABAL",
         "590.423.4560",
         "05-FEB-06",
         "IT_PROG",
         4800,
         " - ",
         "103",
         60
        ],
        [
         107,
         "Diana",
         "Lorentz",
         "DLORENTZ",
         "590.423.5567",
         "07-FEB-07",
         "IT_PROG",
         4200,
         " - ",
         "103",
         60
        ],
        [
         108,
         "Nancy",
         "Greenberg",
         "NGREENBE",
         "515.124.4569",
         "17-AUG-02",
         "FI_MGR",
         12008,
         " - ",
         "101",
         100
        ],
        [
         109,
         "Daniel",
         "Faviet",
         "DFAVIET",
         "515.124.4169",
         "16-AUG-02",
         "FI_ACCOUNT",
         9000,
         " - ",
         "108",
         100
        ],
        [
         110,
         "John",
         "Chen",
         "JCHEN",
         "515.124.4269",
         "28-SEP-05",
         "FI_ACCOUNT",
         8200,
         " - ",
         "108",
         100
        ],
        [
         111,
         "Ismael",
         "Sciarra",
         "ISCIARRA",
         "515.124.4369",
         "30-SEP-05",
         "FI_ACCOUNT",
         7700,
         " - ",
         "108",
         100
        ],
        [
         112,
         "Jose Manuel",
         "Urman",
         "JMURMAN",
         "515.124.4469",
         "07-MAR-06",
         "FI_ACCOUNT",
         7800,
         " - ",
         "108",
         100
        ],
        [
         113,
         "Luis",
         "Popp",
         "LPOPP",
         "515.124.4567",
         "07-DEC-07",
         "FI_ACCOUNT",
         6900,
         " - ",
         "108",
         100
        ],
        [
         114,
         "Den",
         "Raphaely",
         "DRAPHEAL",
         "515.127.4561",
         "07-DEC-02",
         "PU_MAN",
         11000,
         " - ",
         "100",
         30
        ],
        [
         115,
         "Alexander",
         "Khoo",
         "AKHOO",
         "515.127.4562",
         "18-MAY-03",
         "PU_CLERK",
         3100,
         " - ",
         "114",
         30
        ],
        [
         116,
         "Shelli",
         "Baida",
         "SBAIDA",
         "515.127.4563",
         "24-DEC-05",
         "PU_CLERK",
         2900,
         " - ",
         "114",
         30
        ],
        [
         117,
         "Sigal",
         "Tobias",
         "STOBIAS",
         "515.127.4564",
         "24-JUL-05",
         "PU_CLERK",
         2800,
         " - ",
         "114",
         30
        ],
        [
         118,
         "Guy",
         "Himuro",
         "GHIMURO",
         "515.127.4565",
         "15-NOV-06",
         "PU_CLERK",
         2600,
         " - ",
         "114",
         30
        ],
        [
         119,
         "Karen",
         "Colmenares",
         "KCOLMENA",
         "515.127.4566",
         "10-AUG-07",
         "PU_CLERK",
         2500,
         " - ",
         "114",
         30
        ],
        [
         120,
         "Matthew",
         "Weiss",
         "MWEISS",
         "650.123.1234",
         "18-JUL-04",
         "ST_MAN",
         8000,
         " - ",
         "100",
         50
        ],
        [
         121,
         "Adam",
         "Fripp",
         "AFRIPP",
         "650.123.2234",
         "10-APR-05",
         "ST_MAN",
         8200,
         " - ",
         "100",
         50
        ],
        [
         122,
         "Payam",
         "Kaufling",
         "PKAUFLIN",
         "650.123.3234",
         "01-MAY-03",
         "ST_MAN",
         7900,
         " - ",
         "100",
         50
        ],
        [
         123,
         "Shanta",
         "Vollman",
         "SVOLLMAN",
         "650.123.4234",
         "10-OCT-05",
         "ST_MAN",
         6500,
         " - ",
         "100",
         50
        ],
        [
         124,
         "Kevin",
         "Mourgos",
         "KMOURGOS",
         "650.123.5234",
         "16-NOV-07",
         "ST_MAN",
         5800,
         " - ",
         "100",
         50
        ],
        [
         125,
         "Julia",
         "Nayer",
         "JNAYER",
         "650.124.1214",
         "16-JUL-05",
         "ST_CLERK",
         3200,
         " - ",
         "120",
         50
        ],
        [
         126,
         "Irene",
         "Mikkilineni",
         "IMIKKILI",
         "650.124.1224",
         "28-SEP-06",
         "ST_CLERK",
         2700,
         " - ",
         "120",
         50
        ],
        [
         127,
         "James",
         "Landry",
         "JLANDRY",
         "650.124.1334",
         "14-JAN-07",
         "ST_CLERK",
         2400,
         " - ",
         "120",
         50
        ],
        [
         128,
         "Steven",
         "Markle",
         "SMARKLE",
         "650.124.1434",
         "08-MAR-08",
         "ST_CLERK",
         2200,
         " - ",
         "120",
         50
        ],
        [
         129,
         "Laura",
         "Bissot",
         "LBISSOT",
         "650.124.5234",
         "20-AUG-05",
         "ST_CLERK",
         3300,
         " - ",
         "121",
         50
        ],
        [
         130,
         "Mozhe",
         "Atkinson",
         "MATKINSO",
         "650.124.6234",
         "30-OCT-05",
         "ST_CLERK",
         2800,
         " - ",
         "121",
         50
        ],
        [
         131,
         "James",
         "Marlow",
         "JAMRLOW",
         "650.124.7234",
         "16-FEB-05",
         "ST_CLERK",
         2500,
         " - ",
         "121",
         50
        ],
        [
         132,
         "TJ",
         "Olson",
         "TJOLSON",
         "650.124.8234",
         "10-APR-07",
         "ST_CLERK",
         2100,
         " - ",
         "121",
         50
        ],
        [
         133,
         "Jason",
         "Mallin",
         "JMALLIN",
         "650.127.1934",
         "14-JUN-04",
         "ST_CLERK",
         3300,
         " - ",
         "122",
         50
        ],
        [
         134,
         "Michael",
         "Rogers",
         "MROGERS",
         "650.127.1834",
         "26-AUG-06",
         "ST_CLERK",
         2900,
         " - ",
         "122",
         50
        ],
        [
         135,
         "Ki",
         "Gee",
         "KGEE",
         "650.127.1734",
         "12-DEC-07",
         "ST_CLERK",
         2400,
         " - ",
         "122",
         50
        ],
        [
         136,
         "Hazel",
         "Philtanker",
         "HPHILTAN",
         "650.127.1634",
         "06-FEB-08",
         "ST_CLERK",
         2200,
         " - ",
         "122",
         50
        ],
        [
         137,
         "Renske",
         "Ladwig",
         "RLADWIG",
         "650.121.1234",
         "14-JUL-03",
         "ST_CLERK",
         3600,
         " - ",
         "123",
         50
        ],
        [
         138,
         "Stephen",
         "Stiles",
         "SSTILES",
         "650.121.2034",
         "26-OCT-05",
         "ST_CLERK",
         3200,
         " - ",
         "123",
         50
        ],
        [
         139,
         "John",
         "Seo",
         "JSEO",
         "650.121.2019",
         "12-FEB-06",
         "ST_CLERK",
         2700,
         " - ",
         "123",
         50
        ],
        [
         140,
         "Joshua",
         "Patel",
         "JPATEL",
         "650.121.1834",
         "06-APR-06",
         "ST_CLERK",
         2500,
         " - ",
         "123",
         50
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EMPLOYEE_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "FIRST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "LAST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EMAIL",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PHONE_NUMBER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HIRE_DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JOB_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SALARY",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "COMMISSION_PCT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "MANAGER_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEPARTMENT_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46499b54-7505-42b0-a949-af8f4c274fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DEPARTMENT_ID\tDEPARTMENT_NAME\tMANAGER_ID\tLOCATION_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b28722b2-188f-44df-9960-1d34034d367f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65295185-8da7-48f3-a74e-e5fb6f4a613d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DEPARTMENT_ID\tDEPARTMENT_NAME\tMANAGER_ID\tLOCATION_ID\n",
    "\n",
    "# nullable - True - this may contaions null values\n",
    "# False - this fields should not contain any null values \n",
    "\n",
    "dept_schema = StructType([StructField('DEPARTMENT_ID', IntegerType(), True),\n",
    "                         StructField('DEPARTMENT_NAME', StringType(), True),\n",
    "                         StructField('MANAGER_ID', IntegerType(), True),\n",
    "                         StructField('LOCATION_ID', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48659809-179d-4d0c-a994-a629d39a8605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dept_df = spark.read.option(\"header\",True)\\\n",
    "                    .schema(dept_schema)\\\n",
    "                    .csv(\"/Volumes/sample_cat/default/volume_csv/emp_dataset/departments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2016ef3f-a720-4bc6-b8d9-06f834640631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEPARTMENT_ID</th><th>DEPARTMENT_NAME</th><th>MANAGER_ID</th><th>LOCATION_ID</th></tr></thead><tbody><tr><td>10</td><td>Administration</td><td>200</td><td>1700</td></tr><tr><td>20</td><td>Marketing</td><td>201</td><td>1800</td></tr><tr><td>30</td><td>Purchasing</td><td>114</td><td>1700</td></tr><tr><td>40</td><td>Human Resources</td><td>203</td><td>2400</td></tr><tr><td>50</td><td>Shipping</td><td>121</td><td>1500</td></tr><tr><td>60</td><td>IT</td><td>103</td><td>1400</td></tr><tr><td>70</td><td>Public Relations</td><td>204</td><td>2700</td></tr><tr><td>80</td><td>Sales</td><td>145</td><td>2500</td></tr><tr><td>90</td><td>Executive</td><td>100</td><td>1700</td></tr><tr><td>100</td><td>Finance</td><td>108</td><td>1700</td></tr><tr><td>110</td><td>Accounting</td><td>205</td><td>1700</td></tr><tr><td>120</td><td>Treasury</td><td>null</td><td>1700</td></tr><tr><td>130</td><td>Corporate Tax</td><td>null</td><td>1700</td></tr><tr><td>140</td><td>Control And Credit</td><td>null</td><td>1700</td></tr><tr><td>150</td><td>Shareholder Services</td><td>null</td><td>1700</td></tr><tr><td>160</td><td>Benefits</td><td>null</td><td>1700</td></tr><tr><td>170</td><td>Manufacturing</td><td>null</td><td>1700</td></tr><tr><td>180</td><td>Construction</td><td>null</td><td>1700</td></tr><tr><td>190</td><td>Contracting</td><td>null</td><td>1700</td></tr><tr><td>200</td><td>Operations</td><td>null</td><td>1700</td></tr><tr><td>210</td><td>IT Support</td><td>null</td><td>1700</td></tr><tr><td>220</td><td>NOC</td><td>null</td><td>1700</td></tr><tr><td>230</td><td>IT Helpdesk</td><td>null</td><td>1700</td></tr><tr><td>240</td><td>Government Sales</td><td>null</td><td>1700</td></tr><tr><td>250</td><td>Retail Sales</td><td>null</td><td>1700</td></tr><tr><td>260</td><td>Recruiting</td><td>null</td><td>1700</td></tr><tr><td>270</td><td>Payroll</td><td>null</td><td>1700</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Administration",
         200,
         1700
        ],
        [
         20,
         "Marketing",
         201,
         1800
        ],
        [
         30,
         "Purchasing",
         114,
         1700
        ],
        [
         40,
         "Human Resources",
         203,
         2400
        ],
        [
         50,
         "Shipping",
         121,
         1500
        ],
        [
         60,
         "IT",
         103,
         1400
        ],
        [
         70,
         "Public Relations",
         204,
         2700
        ],
        [
         80,
         "Sales",
         145,
         2500
        ],
        [
         90,
         "Executive",
         100,
         1700
        ],
        [
         100,
         "Finance",
         108,
         1700
        ],
        [
         110,
         "Accounting",
         205,
         1700
        ],
        [
         120,
         "Treasury",
         null,
         1700
        ],
        [
         130,
         "Corporate Tax",
         null,
         1700
        ],
        [
         140,
         "Control And Credit",
         null,
         1700
        ],
        [
         150,
         "Shareholder Services",
         null,
         1700
        ],
        [
         160,
         "Benefits",
         null,
         1700
        ],
        [
         170,
         "Manufacturing",
         null,
         1700
        ],
        [
         180,
         "Construction",
         null,
         1700
        ],
        [
         190,
         "Contracting",
         null,
         1700
        ],
        [
         200,
         "Operations",
         null,
         1700
        ],
        [
         210,
         "IT Support",
         null,
         1700
        ],
        [
         220,
         "NOC",
         null,
         1700
        ],
        [
         230,
         "IT Helpdesk",
         null,
         1700
        ],
        [
         240,
         "Government Sales",
         null,
         1700
        ],
        [
         250,
         "Retail Sales",
         null,
         1700
        ],
        [
         260,
         "Recruiting",
         null,
         1700
        ],
        [
         270,
         "Payroll",
         null,
         1700
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEPARTMENT_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DEPARTMENT_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "MANAGER_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "LOCATION_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dept_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6863a626-1ea6-47c6-b61c-a99b56dc4c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6040872030332645>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mcache()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2090\u001B[0m, in \u001B[0;36mDataFrame.cache\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   2089\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcache\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ParentDataFrame:\n",
       "\u001B[0;32m-> 2090\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpersist()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2097\u001B[0m, in \u001B[0;36mDataFrame.persist\u001B[0;34m(self, storageLevel)\u001B[0m\n",
       "\u001B[1;32m   2092\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpersist\u001B[39m(\n",
       "\u001B[1;32m   2093\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m   2094\u001B[0m     storageLevel: StorageLevel \u001B[38;5;241m=\u001B[39m (StorageLevel\u001B[38;5;241m.\u001B[39mMEMORY_AND_DISK_DESER),\n",
       "\u001B[1;32m   2095\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ParentDataFrame:\n",
       "\u001B[1;32m   2096\u001B[0m     relation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mplan(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 2097\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(\n",
       "\u001B[1;32m   2098\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpersist\u001B[39m\u001B[38;5;124m\"\u001B[39m, relation\u001B[38;5;241m=\u001B[39mrelation, storage_level\u001B[38;5;241m=\u001B[39mstorageLevel\n",
       "\u001B[1;32m   2099\u001B[0m     )\n",
       "\u001B[1;32m   2100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1773\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2310\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2308\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2310\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2311\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2312\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2386\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2382\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2384\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2386\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2387\u001B[0m                 info,\n",
       "\u001B[1;32m   2388\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2389\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2390\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2391\u001B[0m                 status\u001B[38;5;241m.\u001B[39mcode,\n",
       "\u001B[1;32m   2392\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2394\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2395\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2396\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2397\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mcode,\n",
       "\u001B[1;32m   2398\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2399\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n",
       "\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:277)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n",
       "\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n",
       "\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n",
       "\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n",
       "\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n",
       "\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:277)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "[NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOT_SUPPORTED_WITH_SERVERLESS",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "0A000",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:277)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-6040872030332645>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mcache()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2090\u001B[0m, in \u001B[0;36mDataFrame.cache\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2089\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcache\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ParentDataFrame:\n\u001B[0;32m-> 2090\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpersist()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2097\u001B[0m, in \u001B[0;36mDataFrame.persist\u001B[0;34m(self, storageLevel)\u001B[0m\n\u001B[1;32m   2092\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpersist\u001B[39m(\n\u001B[1;32m   2093\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   2094\u001B[0m     storageLevel: StorageLevel \u001B[38;5;241m=\u001B[39m (StorageLevel\u001B[38;5;241m.\u001B[39mMEMORY_AND_DISK_DESER),\n\u001B[1;32m   2095\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ParentDataFrame:\n\u001B[1;32m   2096\u001B[0m     relation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mplan(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 2097\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(\n\u001B[1;32m   2098\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpersist\u001B[39m\u001B[38;5;124m\"\u001B[39m, relation\u001B[38;5;241m=\u001B[39mrelation, storage_level\u001B[38;5;241m=\u001B[39mstorageLevel\n\u001B[1;32m   2099\u001B[0m     )\n\u001B[1;32m   2100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1773\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2310\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2308\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2310\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2311\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2312\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2386\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2382\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2384\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2386\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2387\u001B[0m                 info,\n\u001B[1;32m   2388\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2389\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2390\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2391\u001B[0m                 status\u001B[38;5;241m.\u001B[39mcode,\n\u001B[1;32m   2392\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2394\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2395\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2396\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2397\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mcode,\n\u001B[1;32m   2398\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2399\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:277)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cache is used to store or cache the data in memory\n",
    "\n",
    "emp_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e65dda-d319-4960-a169-659519b389a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6040872030332646>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mpersist()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2097\u001B[0m, in \u001B[0;36mDataFrame.persist\u001B[0;34m(self, storageLevel)\u001B[0m\n",
       "\u001B[1;32m   2092\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpersist\u001B[39m(\n",
       "\u001B[1;32m   2093\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m   2094\u001B[0m     storageLevel: StorageLevel \u001B[38;5;241m=\u001B[39m (StorageLevel\u001B[38;5;241m.\u001B[39mMEMORY_AND_DISK_DESER),\n",
       "\u001B[1;32m   2095\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ParentDataFrame:\n",
       "\u001B[1;32m   2096\u001B[0m     relation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mplan(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 2097\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(\n",
       "\u001B[1;32m   2098\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpersist\u001B[39m\u001B[38;5;124m\"\u001B[39m, relation\u001B[38;5;241m=\u001B[39mrelation, storage_level\u001B[38;5;241m=\u001B[39mstorageLevel\n",
       "\u001B[1;32m   2099\u001B[0m     )\n",
       "\u001B[1;32m   2100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1773\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2310\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2308\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2310\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2311\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2312\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2386\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2382\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2384\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2386\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2387\u001B[0m                 info,\n",
       "\u001B[1;32m   2388\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2389\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2390\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2391\u001B[0m                 status\u001B[38;5;241m.\u001B[39mcode,\n",
       "\u001B[1;32m   2392\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2394\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2395\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2396\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2397\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mcode,\n",
       "\u001B[1;32m   2398\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2399\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n",
       "\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:277)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n",
       "\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n",
       "\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n",
       "\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n",
       "\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n",
       "\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:277)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "[NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOT_SUPPORTED_WITH_SERVERLESS",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "0A000",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:277)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-6040872030332646>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mpersist()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2097\u001B[0m, in \u001B[0;36mDataFrame.persist\u001B[0;34m(self, storageLevel)\u001B[0m\n\u001B[1;32m   2092\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpersist\u001B[39m(\n\u001B[1;32m   2093\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   2094\u001B[0m     storageLevel: StorageLevel \u001B[38;5;241m=\u001B[39m (StorageLevel\u001B[38;5;241m.\u001B[39mMEMORY_AND_DISK_DESER),\n\u001B[1;32m   2095\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ParentDataFrame:\n\u001B[1;32m   2096\u001B[0m     relation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mplan(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 2097\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(\n\u001B[1;32m   2098\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpersist\u001B[39m\u001B[38;5;124m\"\u001B[39m, relation\u001B[38;5;241m=\u001B[39mrelation, storage_level\u001B[38;5;241m=\u001B[39mstorageLevel\n\u001B[1;32m   2099\u001B[0m     )\n\u001B[1;32m   2100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1773\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2310\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2308\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2310\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2311\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2312\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2386\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2382\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2384\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2386\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2387\u001B[0m                 info,\n\u001B[1;32m   2388\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2389\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2390\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2391\u001B[0m                 status\u001B[38;5;241m.\u001B[39mcode,\n\u001B[1;32m   2392\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2394\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2395\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2396\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2397\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mcode,\n\u001B[1;32m   2398\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2399\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:277)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2349ee4a-3af5-450e-bce3-e792e0b3716c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6040872030332647>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m emp_df\u001B[38;5;241m.\u001B[39munpersist()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2113\u001B[0m, in \u001B[0;36mDataFrame.unpersist\u001B[0;34m(self, blocking)\u001B[0m\n",
       "\u001B[1;32m   2111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munpersist\u001B[39m(\u001B[38;5;28mself\u001B[39m, blocking: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ParentDataFrame:\n",
       "\u001B[1;32m   2112\u001B[0m     relation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mplan(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 2113\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munpersist\u001B[39m\u001B[38;5;124m\"\u001B[39m, relation\u001B[38;5;241m=\u001B[39mrelation, blocking\u001B[38;5;241m=\u001B[39mblocking)\n",
       "\u001B[1;32m   2114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1773\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2310\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2308\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2310\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2311\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2312\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2386\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2382\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2384\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2386\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2387\u001B[0m                 info,\n",
       "\u001B[1;32m   2388\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2389\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2390\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2391\u001B[0m                 status\u001B[38;5;241m.\u001B[39mcode,\n",
       "\u001B[1;32m   2392\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2394\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2395\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2396\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2397\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mcode,\n",
       "\u001B[1;32m   2398\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2399\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [NOT_SUPPORTED_WITH_SERVERLESS] UNPERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n",
       "\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:306)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n",
       "\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n",
       "\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n",
       "\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n",
       "\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n",
       "\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[NOT_SUPPORTED_WITH_SERVERLESS] UNPERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:306)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "[NOT_SUPPORTED_WITH_SERVERLESS] UNPERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOT_SUPPORTED_WITH_SERVERLESS",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "0A000",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:306)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-6040872030332647>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m emp_df\u001B[38;5;241m.\u001B[39munpersist()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2113\u001B[0m, in \u001B[0;36mDataFrame.unpersist\u001B[0;34m(self, blocking)\u001B[0m\n\u001B[1;32m   2111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munpersist\u001B[39m(\u001B[38;5;28mself\u001B[39m, blocking: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ParentDataFrame:\n\u001B[1;32m   2112\u001B[0m     relation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mplan(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 2113\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munpersist\u001B[39m\u001B[38;5;124m\"\u001B[39m, relation\u001B[38;5;241m=\u001B[39mrelation, blocking\u001B[38;5;241m=\u001B[39mblocking)\n\u001B[1;32m   2114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1773\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2310\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2308\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2310\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2311\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2312\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2386\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2382\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2384\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2386\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2387\u001B[0m                 info,\n\u001B[1;32m   2388\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2389\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2390\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2391\u001B[0m                 status\u001B[38;5;241m.\u001B[39mcode,\n\u001B[1;32m   2392\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2394\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2395\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2396\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2397\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mcode,\n\u001B[1;32m   2398\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2399\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [NOT_SUPPORTED_WITH_SERVERLESS] UNPERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:306)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:113)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unpersist is used to uncache the data from memory\n",
    "\n",
    "emp_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f9a06e-c683-47a4-a76d-ec8c30c132b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nFileScan csv [EMPLOYEE_ID#11034,FIRST_NAME#11035,LAST_NAME#11036,EMAIL#11037,PHONE_NUMBER#11038,HIRE_DATE#11039,JOB_ID#11040,SALARY#11041,COMMISSION_PCT#11042,MANAGER_ID#11043,DEPARTMENT_ID#11044] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/sample_cat/default/volume_csv/emp_dataset/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<EMPLOYEE_ID:int,FIRST_NAME:string,LAST_NAME:string,EMAIL:string,PHONE_NUMBER:string,HIRE_D...\n\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ac6bec-76b2-4a93-89f6-5cfc8b68a6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nFileScan csv [DEPARTMENT_ID#11059,DEPARTMENT_NAME#11060,MANAGER_ID#11061,LOCATION_ID#11062] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/sample_cat/default/volume_csv/emp_dataset/departments.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEPARTMENT_ID:int,DEPARTMENT_NAME:string,MANAGER_ID:int,LOCATION_ID:int>\n\n\n"
     ]
    }
   ],
   "source": [
    "dept_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05f6cadb-553e-42b5-abfb-6c3911b3a61e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# partitions ----> parts\n",
    "\n",
    "two methods\n",
    "\n",
    "repartition()   --> if we wan to increase partition size then we can use repartition\n",
    "coalesce()    ----> coalesce used to decrease partitons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a289db8-b7b6-4336-8f0d-ff23989bd8ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc2157f-aeaa-40a2-8575-035c295798f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EMPLOYEE_ID</th><th>FIRST_NAME</th><th>LAST_NAME</th><th>EMAIL</th><th>PHONE_NUMBER</th><th>HIRE_DATE</th><th>JOB_ID</th><th>SALARY</th><th>COMMISSION_PCT</th><th>MANAGER_ID</th><th>DEPARTMENT_ID</th></tr></thead><tbody><tr><td>198</td><td>Donald</td><td>OConnell</td><td>DOCONNEL</td><td>650.507.9833</td><td>21-JUN-07</td><td>SH_CLERK</td><td>2600</td><td> - </td><td>124</td><td>50</td></tr><tr><td>199</td><td>Douglas</td><td>Grant</td><td>DGRANT</td><td>650.507.9844</td><td>13-JAN-08</td><td>SH_CLERK</td><td>2600</td><td> - </td><td>124</td><td>50</td></tr><tr><td>200</td><td>Jennifer</td><td>Whalen</td><td>JWHALEN</td><td>515.123.4444</td><td>17-SEP-03</td><td>AD_ASST</td><td>4400</td><td> - </td><td>101</td><td>10</td></tr><tr><td>201</td><td>Michael</td><td>Hartstein</td><td>MHARTSTE</td><td>515.123.5555</td><td>17-FEB-04</td><td>MK_MAN</td><td>13000</td><td> - </td><td>100</td><td>20</td></tr><tr><td>202</td><td>Pat</td><td>Fay</td><td>PFAY</td><td>603.123.6666</td><td>17-AUG-05</td><td>MK_REP</td><td>6000</td><td> - </td><td>201</td><td>20</td></tr><tr><td>203</td><td>Susan</td><td>Mavris</td><td>SMAVRIS</td><td>515.123.7777</td><td>07-JUN-02</td><td>HR_REP</td><td>6500</td><td> - </td><td>101</td><td>40</td></tr><tr><td>204</td><td>Hermann</td><td>Baer</td><td>HBAER</td><td>515.123.8888</td><td>07-JUN-02</td><td>PR_REP</td><td>10000</td><td> - </td><td>101</td><td>70</td></tr><tr><td>205</td><td>Shelley</td><td>Higgins</td><td>SHIGGINS</td><td>515.123.8080</td><td>07-JUN-02</td><td>AC_MGR</td><td>12008</td><td> - </td><td>101</td><td>110</td></tr><tr><td>206</td><td>William</td><td>Gietz</td><td>WGIETZ</td><td>515.123.8181</td><td>07-JUN-02</td><td>AC_ACCOUNT</td><td>8300</td><td> - </td><td>205</td><td>110</td></tr><tr><td>100</td><td>Steven</td><td>King</td><td>SKING</td><td>515.123.4567</td><td>17-JUN-03</td><td>AD_PRES</td><td>24000</td><td> - </td><td> - </td><td>90</td></tr><tr><td>101</td><td>Neena</td><td>Kochhar</td><td>NKOCHHAR</td><td>515.123.4568</td><td>21-SEP-05</td><td>AD_VP</td><td>17000</td><td> - </td><td>100</td><td>90</td></tr><tr><td>102</td><td>Lex</td><td>De Haan</td><td>LDEHAAN</td><td>515.123.4569</td><td>13-JAN-01</td><td>AD_VP</td><td>17000</td><td> - </td><td>100</td><td>90</td></tr><tr><td>103</td><td>Alexander</td><td>Hunold</td><td>AHUNOLD</td><td>590.423.4567</td><td>03-JAN-06</td><td>IT_PROG</td><td>9000</td><td> - </td><td>102</td><td>60</td></tr><tr><td>104</td><td>Bruce</td><td>Ernst</td><td>BERNST</td><td>590.423.4568</td><td>21-MAY-07</td><td>IT_PROG</td><td>6000</td><td> - </td><td>103</td><td>60</td></tr><tr><td>105</td><td>David</td><td>Austin</td><td>DAUSTIN</td><td>590.423.4569</td><td>25-JUN-05</td><td>IT_PROG</td><td>4800</td><td> - </td><td>103</td><td>60</td></tr><tr><td>106</td><td>Valli</td><td>Pataballa</td><td>VPATABAL</td><td>590.423.4560</td><td>05-FEB-06</td><td>IT_PROG</td><td>4800</td><td> - </td><td>103</td><td>60</td></tr><tr><td>107</td><td>Diana</td><td>Lorentz</td><td>DLORENTZ</td><td>590.423.5567</td><td>07-FEB-07</td><td>IT_PROG</td><td>4200</td><td> - </td><td>103</td><td>60</td></tr><tr><td>108</td><td>Nancy</td><td>Greenberg</td><td>NGREENBE</td><td>515.124.4569</td><td>17-AUG-02</td><td>FI_MGR</td><td>12008</td><td> - </td><td>101</td><td>100</td></tr><tr><td>109</td><td>Daniel</td><td>Faviet</td><td>DFAVIET</td><td>515.124.4169</td><td>16-AUG-02</td><td>FI_ACCOUNT</td><td>9000</td><td> - </td><td>108</td><td>100</td></tr><tr><td>110</td><td>John</td><td>Chen</td><td>JCHEN</td><td>515.124.4269</td><td>28-SEP-05</td><td>FI_ACCOUNT</td><td>8200</td><td> - </td><td>108</td><td>100</td></tr><tr><td>111</td><td>Ismael</td><td>Sciarra</td><td>ISCIARRA</td><td>515.124.4369</td><td>30-SEP-05</td><td>FI_ACCOUNT</td><td>7700</td><td> - </td><td>108</td><td>100</td></tr><tr><td>112</td><td>Jose Manuel</td><td>Urman</td><td>JMURMAN</td><td>515.124.4469</td><td>07-MAR-06</td><td>FI_ACCOUNT</td><td>7800</td><td> - </td><td>108</td><td>100</td></tr><tr><td>113</td><td>Luis</td><td>Popp</td><td>LPOPP</td><td>515.124.4567</td><td>07-DEC-07</td><td>FI_ACCOUNT</td><td>6900</td><td> - </td><td>108</td><td>100</td></tr><tr><td>114</td><td>Den</td><td>Raphaely</td><td>DRAPHEAL</td><td>515.127.4561</td><td>07-DEC-02</td><td>PU_MAN</td><td>11000</td><td> - </td><td>100</td><td>30</td></tr><tr><td>115</td><td>Alexander</td><td>Khoo</td><td>AKHOO</td><td>515.127.4562</td><td>18-MAY-03</td><td>PU_CLERK</td><td>3100</td><td> - </td><td>114</td><td>30</td></tr><tr><td>116</td><td>Shelli</td><td>Baida</td><td>SBAIDA</td><td>515.127.4563</td><td>24-DEC-05</td><td>PU_CLERK</td><td>2900</td><td> - </td><td>114</td><td>30</td></tr><tr><td>117</td><td>Sigal</td><td>Tobias</td><td>STOBIAS</td><td>515.127.4564</td><td>24-JUL-05</td><td>PU_CLERK</td><td>2800</td><td> - </td><td>114</td><td>30</td></tr><tr><td>118</td><td>Guy</td><td>Himuro</td><td>GHIMURO</td><td>515.127.4565</td><td>15-NOV-06</td><td>PU_CLERK</td><td>2600</td><td> - </td><td>114</td><td>30</td></tr><tr><td>119</td><td>Karen</td><td>Colmenares</td><td>KCOLMENA</td><td>515.127.4566</td><td>10-AUG-07</td><td>PU_CLERK</td><td>2500</td><td> - </td><td>114</td><td>30</td></tr><tr><td>120</td><td>Matthew</td><td>Weiss</td><td>MWEISS</td><td>650.123.1234</td><td>18-JUL-04</td><td>ST_MAN</td><td>8000</td><td> - </td><td>100</td><td>50</td></tr><tr><td>121</td><td>Adam</td><td>Fripp</td><td>AFRIPP</td><td>650.123.2234</td><td>10-APR-05</td><td>ST_MAN</td><td>8200</td><td> - </td><td>100</td><td>50</td></tr><tr><td>122</td><td>Payam</td><td>Kaufling</td><td>PKAUFLIN</td><td>650.123.3234</td><td>01-MAY-03</td><td>ST_MAN</td><td>7900</td><td> - </td><td>100</td><td>50</td></tr><tr><td>123</td><td>Shanta</td><td>Vollman</td><td>SVOLLMAN</td><td>650.123.4234</td><td>10-OCT-05</td><td>ST_MAN</td><td>6500</td><td> - </td><td>100</td><td>50</td></tr><tr><td>124</td><td>Kevin</td><td>Mourgos</td><td>KMOURGOS</td><td>650.123.5234</td><td>16-NOV-07</td><td>ST_MAN</td><td>5800</td><td> - </td><td>100</td><td>50</td></tr><tr><td>125</td><td>Julia</td><td>Nayer</td><td>JNAYER</td><td>650.124.1214</td><td>16-JUL-05</td><td>ST_CLERK</td><td>3200</td><td> - </td><td>120</td><td>50</td></tr><tr><td>126</td><td>Irene</td><td>Mikkilineni</td><td>IMIKKILI</td><td>650.124.1224</td><td>28-SEP-06</td><td>ST_CLERK</td><td>2700</td><td> - </td><td>120</td><td>50</td></tr><tr><td>127</td><td>James</td><td>Landry</td><td>JLANDRY</td><td>650.124.1334</td><td>14-JAN-07</td><td>ST_CLERK</td><td>2400</td><td> - </td><td>120</td><td>50</td></tr><tr><td>128</td><td>Steven</td><td>Markle</td><td>SMARKLE</td><td>650.124.1434</td><td>08-MAR-08</td><td>ST_CLERK</td><td>2200</td><td> - </td><td>120</td><td>50</td></tr><tr><td>129</td><td>Laura</td><td>Bissot</td><td>LBISSOT</td><td>650.124.5234</td><td>20-AUG-05</td><td>ST_CLERK</td><td>3300</td><td> - </td><td>121</td><td>50</td></tr><tr><td>130</td><td>Mozhe</td><td>Atkinson</td><td>MATKINSO</td><td>650.124.6234</td><td>30-OCT-05</td><td>ST_CLERK</td><td>2800</td><td> - </td><td>121</td><td>50</td></tr><tr><td>131</td><td>James</td><td>Marlow</td><td>JAMRLOW</td><td>650.124.7234</td><td>16-FEB-05</td><td>ST_CLERK</td><td>2500</td><td> - </td><td>121</td><td>50</td></tr><tr><td>132</td><td>TJ</td><td>Olson</td><td>TJOLSON</td><td>650.124.8234</td><td>10-APR-07</td><td>ST_CLERK</td><td>2100</td><td> - </td><td>121</td><td>50</td></tr><tr><td>133</td><td>Jason</td><td>Mallin</td><td>JMALLIN</td><td>650.127.1934</td><td>14-JUN-04</td><td>ST_CLERK</td><td>3300</td><td> - </td><td>122</td><td>50</td></tr><tr><td>134</td><td>Michael</td><td>Rogers</td><td>MROGERS</td><td>650.127.1834</td><td>26-AUG-06</td><td>ST_CLERK</td><td>2900</td><td> - </td><td>122</td><td>50</td></tr><tr><td>135</td><td>Ki</td><td>Gee</td><td>KGEE</td><td>650.127.1734</td><td>12-DEC-07</td><td>ST_CLERK</td><td>2400</td><td> - </td><td>122</td><td>50</td></tr><tr><td>136</td><td>Hazel</td><td>Philtanker</td><td>HPHILTAN</td><td>650.127.1634</td><td>06-FEB-08</td><td>ST_CLERK</td><td>2200</td><td> - </td><td>122</td><td>50</td></tr><tr><td>137</td><td>Renske</td><td>Ladwig</td><td>RLADWIG</td><td>650.121.1234</td><td>14-JUL-03</td><td>ST_CLERK</td><td>3600</td><td> - </td><td>123</td><td>50</td></tr><tr><td>138</td><td>Stephen</td><td>Stiles</td><td>SSTILES</td><td>650.121.2034</td><td>26-OCT-05</td><td>ST_CLERK</td><td>3200</td><td> - </td><td>123</td><td>50</td></tr><tr><td>139</td><td>John</td><td>Seo</td><td>JSEO</td><td>650.121.2019</td><td>12-FEB-06</td><td>ST_CLERK</td><td>2700</td><td> - </td><td>123</td><td>50</td></tr><tr><td>140</td><td>Joshua</td><td>Patel</td><td>JPATEL</td><td>650.121.1834</td><td>06-APR-06</td><td>ST_CLERK</td><td>2500</td><td> - </td><td>123</td><td>50</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         198,
         "Donald",
         "OConnell",
         "DOCONNEL",
         "650.507.9833",
         "21-JUN-07",
         "SH_CLERK",
         2600,
         " - ",
         "124",
         50
        ],
        [
         199,
         "Douglas",
         "Grant",
         "DGRANT",
         "650.507.9844",
         "13-JAN-08",
         "SH_CLERK",
         2600,
         " - ",
         "124",
         50
        ],
        [
         200,
         "Jennifer",
         "Whalen",
         "JWHALEN",
         "515.123.4444",
         "17-SEP-03",
         "AD_ASST",
         4400,
         " - ",
         "101",
         10
        ],
        [
         201,
         "Michael",
         "Hartstein",
         "MHARTSTE",
         "515.123.5555",
         "17-FEB-04",
         "MK_MAN",
         13000,
         " - ",
         "100",
         20
        ],
        [
         202,
         "Pat",
         "Fay",
         "PFAY",
         "603.123.6666",
         "17-AUG-05",
         "MK_REP",
         6000,
         " - ",
         "201",
         20
        ],
        [
         203,
         "Susan",
         "Mavris",
         "SMAVRIS",
         "515.123.7777",
         "07-JUN-02",
         "HR_REP",
         6500,
         " - ",
         "101",
         40
        ],
        [
         204,
         "Hermann",
         "Baer",
         "HBAER",
         "515.123.8888",
         "07-JUN-02",
         "PR_REP",
         10000,
         " - ",
         "101",
         70
        ],
        [
         205,
         "Shelley",
         "Higgins",
         "SHIGGINS",
         "515.123.8080",
         "07-JUN-02",
         "AC_MGR",
         12008,
         " - ",
         "101",
         110
        ],
        [
         206,
         "William",
         "Gietz",
         "WGIETZ",
         "515.123.8181",
         "07-JUN-02",
         "AC_ACCOUNT",
         8300,
         " - ",
         "205",
         110
        ],
        [
         100,
         "Steven",
         "King",
         "SKING",
         "515.123.4567",
         "17-JUN-03",
         "AD_PRES",
         24000,
         " - ",
         " - ",
         90
        ],
        [
         101,
         "Neena",
         "Kochhar",
         "NKOCHHAR",
         "515.123.4568",
         "21-SEP-05",
         "AD_VP",
         17000,
         " - ",
         "100",
         90
        ],
        [
         102,
         "Lex",
         "De Haan",
         "LDEHAAN",
         "515.123.4569",
         "13-JAN-01",
         "AD_VP",
         17000,
         " - ",
         "100",
         90
        ],
        [
         103,
         "Alexander",
         "Hunold",
         "AHUNOLD",
         "590.423.4567",
         "03-JAN-06",
         "IT_PROG",
         9000,
         " - ",
         "102",
         60
        ],
        [
         104,
         "Bruce",
         "Ernst",
         "BERNST",
         "590.423.4568",
         "21-MAY-07",
         "IT_PROG",
         6000,
         " - ",
         "103",
         60
        ],
        [
         105,
         "David",
         "Austin",
         "DAUSTIN",
         "590.423.4569",
         "25-JUN-05",
         "IT_PROG",
         4800,
         " - ",
         "103",
         60
        ],
        [
         106,
         "Valli",
         "Pataballa",
         "VPATABAL",
         "590.423.4560",
         "05-FEB-06",
         "IT_PROG",
         4800,
         " - ",
         "103",
         60
        ],
        [
         107,
         "Diana",
         "Lorentz",
         "DLORENTZ",
         "590.423.5567",
         "07-FEB-07",
         "IT_PROG",
         4200,
         " - ",
         "103",
         60
        ],
        [
         108,
         "Nancy",
         "Greenberg",
         "NGREENBE",
         "515.124.4569",
         "17-AUG-02",
         "FI_MGR",
         12008,
         " - ",
         "101",
         100
        ],
        [
         109,
         "Daniel",
         "Faviet",
         "DFAVIET",
         "515.124.4169",
         "16-AUG-02",
         "FI_ACCOUNT",
         9000,
         " - ",
         "108",
         100
        ],
        [
         110,
         "John",
         "Chen",
         "JCHEN",
         "515.124.4269",
         "28-SEP-05",
         "FI_ACCOUNT",
         8200,
         " - ",
         "108",
         100
        ],
        [
         111,
         "Ismael",
         "Sciarra",
         "ISCIARRA",
         "515.124.4369",
         "30-SEP-05",
         "FI_ACCOUNT",
         7700,
         " - ",
         "108",
         100
        ],
        [
         112,
         "Jose Manuel",
         "Urman",
         "JMURMAN",
         "515.124.4469",
         "07-MAR-06",
         "FI_ACCOUNT",
         7800,
         " - ",
         "108",
         100
        ],
        [
         113,
         "Luis",
         "Popp",
         "LPOPP",
         "515.124.4567",
         "07-DEC-07",
         "FI_ACCOUNT",
         6900,
         " - ",
         "108",
         100
        ],
        [
         114,
         "Den",
         "Raphaely",
         "DRAPHEAL",
         "515.127.4561",
         "07-DEC-02",
         "PU_MAN",
         11000,
         " - ",
         "100",
         30
        ],
        [
         115,
         "Alexander",
         "Khoo",
         "AKHOO",
         "515.127.4562",
         "18-MAY-03",
         "PU_CLERK",
         3100,
         " - ",
         "114",
         30
        ],
        [
         116,
         "Shelli",
         "Baida",
         "SBAIDA",
         "515.127.4563",
         "24-DEC-05",
         "PU_CLERK",
         2900,
         " - ",
         "114",
         30
        ],
        [
         117,
         "Sigal",
         "Tobias",
         "STOBIAS",
         "515.127.4564",
         "24-JUL-05",
         "PU_CLERK",
         2800,
         " - ",
         "114",
         30
        ],
        [
         118,
         "Guy",
         "Himuro",
         "GHIMURO",
         "515.127.4565",
         "15-NOV-06",
         "PU_CLERK",
         2600,
         " - ",
         "114",
         30
        ],
        [
         119,
         "Karen",
         "Colmenares",
         "KCOLMENA",
         "515.127.4566",
         "10-AUG-07",
         "PU_CLERK",
         2500,
         " - ",
         "114",
         30
        ],
        [
         120,
         "Matthew",
         "Weiss",
         "MWEISS",
         "650.123.1234",
         "18-JUL-04",
         "ST_MAN",
         8000,
         " - ",
         "100",
         50
        ],
        [
         121,
         "Adam",
         "Fripp",
         "AFRIPP",
         "650.123.2234",
         "10-APR-05",
         "ST_MAN",
         8200,
         " - ",
         "100",
         50
        ],
        [
         122,
         "Payam",
         "Kaufling",
         "PKAUFLIN",
         "650.123.3234",
         "01-MAY-03",
         "ST_MAN",
         7900,
         " - ",
         "100",
         50
        ],
        [
         123,
         "Shanta",
         "Vollman",
         "SVOLLMAN",
         "650.123.4234",
         "10-OCT-05",
         "ST_MAN",
         6500,
         " - ",
         "100",
         50
        ],
        [
         124,
         "Kevin",
         "Mourgos",
         "KMOURGOS",
         "650.123.5234",
         "16-NOV-07",
         "ST_MAN",
         5800,
         " - ",
         "100",
         50
        ],
        [
         125,
         "Julia",
         "Nayer",
         "JNAYER",
         "650.124.1214",
         "16-JUL-05",
         "ST_CLERK",
         3200,
         " - ",
         "120",
         50
        ],
        [
         126,
         "Irene",
         "Mikkilineni",
         "IMIKKILI",
         "650.124.1224",
         "28-SEP-06",
         "ST_CLERK",
         2700,
         " - ",
         "120",
         50
        ],
        [
         127,
         "James",
         "Landry",
         "JLANDRY",
         "650.124.1334",
         "14-JAN-07",
         "ST_CLERK",
         2400,
         " - ",
         "120",
         50
        ],
        [
         128,
         "Steven",
         "Markle",
         "SMARKLE",
         "650.124.1434",
         "08-MAR-08",
         "ST_CLERK",
         2200,
         " - ",
         "120",
         50
        ],
        [
         129,
         "Laura",
         "Bissot",
         "LBISSOT",
         "650.124.5234",
         "20-AUG-05",
         "ST_CLERK",
         3300,
         " - ",
         "121",
         50
        ],
        [
         130,
         "Mozhe",
         "Atkinson",
         "MATKINSO",
         "650.124.6234",
         "30-OCT-05",
         "ST_CLERK",
         2800,
         " - ",
         "121",
         50
        ],
        [
         131,
         "James",
         "Marlow",
         "JAMRLOW",
         "650.124.7234",
         "16-FEB-05",
         "ST_CLERK",
         2500,
         " - ",
         "121",
         50
        ],
        [
         132,
         "TJ",
         "Olson",
         "TJOLSON",
         "650.124.8234",
         "10-APR-07",
         "ST_CLERK",
         2100,
         " - ",
         "121",
         50
        ],
        [
         133,
         "Jason",
         "Mallin",
         "JMALLIN",
         "650.127.1934",
         "14-JUN-04",
         "ST_CLERK",
         3300,
         " - ",
         "122",
         50
        ],
        [
         134,
         "Michael",
         "Rogers",
         "MROGERS",
         "650.127.1834",
         "26-AUG-06",
         "ST_CLERK",
         2900,
         " - ",
         "122",
         50
        ],
        [
         135,
         "Ki",
         "Gee",
         "KGEE",
         "650.127.1734",
         "12-DEC-07",
         "ST_CLERK",
         2400,
         " - ",
         "122",
         50
        ],
        [
         136,
         "Hazel",
         "Philtanker",
         "HPHILTAN",
         "650.127.1634",
         "06-FEB-08",
         "ST_CLERK",
         2200,
         " - ",
         "122",
         50
        ],
        [
         137,
         "Renske",
         "Ladwig",
         "RLADWIG",
         "650.121.1234",
         "14-JUL-03",
         "ST_CLERK",
         3600,
         " - ",
         "123",
         50
        ],
        [
         138,
         "Stephen",
         "Stiles",
         "SSTILES",
         "650.121.2034",
         "26-OCT-05",
         "ST_CLERK",
         3200,
         " - ",
         "123",
         50
        ],
        [
         139,
         "John",
         "Seo",
         "JSEO",
         "650.121.2019",
         "12-FEB-06",
         "ST_CLERK",
         2700,
         " - ",
         "123",
         50
        ],
        [
         140,
         "Joshua",
         "Patel",
         "JPATEL",
         "650.121.1834",
         "06-APR-06",
         "ST_CLERK",
         2500,
         " - ",
         "123",
         50
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EMPLOYEE_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "FIRST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "LAST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EMAIL",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PHONE_NUMBER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HIRE_DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JOB_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SALARY",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "COMMISSION_PCT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "MANAGER_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEPARTMENT_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_df.filter(col(\"SALARY\")>200).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d10eef-f389-4187-8a95-3223114dc650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkNotImplementedError\u001B[0m                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6040872030332652>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mgetNumPartitions()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2327\u001B[0m, in \u001B[0;36mDataFrame.rdd\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   2325\u001B[0m \u001B[38;5;129m@property\u001B[39m\n",
       "\u001B[1;32m   2326\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrdd\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Row]\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m-> 2327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkNotImplementedError(\n",
       "\u001B[1;32m   2328\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_IMPLEMENTED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2329\u001B[0m         messageParameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n",
       "\u001B[1;32m   2330\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mPySparkNotImplementedError\u001B[0m: [NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkNotImplementedError",
        "evalue": "[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       },
       "metadata": {
        "errorSummary": "[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOT_IMPLEMENTED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkNotImplementedError\u001B[0m                Traceback (most recent call last)",
        "File \u001B[0;32m<command-6040872030332652>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mgetNumPartitions()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2327\u001B[0m, in \u001B[0;36mDataFrame.rdd\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2325\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   2326\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrdd\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Row]\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 2327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkNotImplementedError(\n\u001B[1;32m   2328\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_IMPLEMENTED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2329\u001B[0m         messageParameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m   2330\u001B[0m     )\n",
        "\u001B[0;31mPySparkNotImplementedError\u001B[0m: [NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How do we know how many partitions we have on our dataframe ?\n",
    "\n",
    "emp_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3683ce1a-a2e6-4bf8-86ed-a34d152d6199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_part_df = emp_df.withColumn('pid',spark_partition_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f84364e-90eb-4053-b398-db99c1fcfb6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EMPLOYEE_ID</th><th>FIRST_NAME</th><th>LAST_NAME</th><th>EMAIL</th><th>PHONE_NUMBER</th><th>HIRE_DATE</th><th>JOB_ID</th><th>SALARY</th><th>COMMISSION_PCT</th><th>MANAGER_ID</th><th>DEPARTMENT_ID</th><th>pid</th></tr></thead><tbody><tr><td>198</td><td>Donald</td><td>OConnell</td><td>DOCONNEL</td><td>650.507.9833</td><td>21-JUN-07</td><td>SH_CLERK</td><td>2600</td><td> - </td><td>124</td><td>50</td><td>0</td></tr><tr><td>199</td><td>Douglas</td><td>Grant</td><td>DGRANT</td><td>650.507.9844</td><td>13-JAN-08</td><td>SH_CLERK</td><td>2600</td><td> - </td><td>124</td><td>50</td><td>0</td></tr><tr><td>200</td><td>Jennifer</td><td>Whalen</td><td>JWHALEN</td><td>515.123.4444</td><td>17-SEP-03</td><td>AD_ASST</td><td>4400</td><td> - </td><td>101</td><td>10</td><td>0</td></tr><tr><td>201</td><td>Michael</td><td>Hartstein</td><td>MHARTSTE</td><td>515.123.5555</td><td>17-FEB-04</td><td>MK_MAN</td><td>13000</td><td> - </td><td>100</td><td>20</td><td>0</td></tr><tr><td>202</td><td>Pat</td><td>Fay</td><td>PFAY</td><td>603.123.6666</td><td>17-AUG-05</td><td>MK_REP</td><td>6000</td><td> - </td><td>201</td><td>20</td><td>0</td></tr><tr><td>203</td><td>Susan</td><td>Mavris</td><td>SMAVRIS</td><td>515.123.7777</td><td>07-JUN-02</td><td>HR_REP</td><td>6500</td><td> - </td><td>101</td><td>40</td><td>0</td></tr><tr><td>204</td><td>Hermann</td><td>Baer</td><td>HBAER</td><td>515.123.8888</td><td>07-JUN-02</td><td>PR_REP</td><td>10000</td><td> - </td><td>101</td><td>70</td><td>0</td></tr><tr><td>205</td><td>Shelley</td><td>Higgins</td><td>SHIGGINS</td><td>515.123.8080</td><td>07-JUN-02</td><td>AC_MGR</td><td>12008</td><td> - </td><td>101</td><td>110</td><td>0</td></tr><tr><td>206</td><td>William</td><td>Gietz</td><td>WGIETZ</td><td>515.123.8181</td><td>07-JUN-02</td><td>AC_ACCOUNT</td><td>8300</td><td> - </td><td>205</td><td>110</td><td>0</td></tr><tr><td>100</td><td>Steven</td><td>King</td><td>SKING</td><td>515.123.4567</td><td>17-JUN-03</td><td>AD_PRES</td><td>24000</td><td> - </td><td> - </td><td>90</td><td>0</td></tr><tr><td>101</td><td>Neena</td><td>Kochhar</td><td>NKOCHHAR</td><td>515.123.4568</td><td>21-SEP-05</td><td>AD_VP</td><td>17000</td><td> - </td><td>100</td><td>90</td><td>0</td></tr><tr><td>102</td><td>Lex</td><td>De Haan</td><td>LDEHAAN</td><td>515.123.4569</td><td>13-JAN-01</td><td>AD_VP</td><td>17000</td><td> - </td><td>100</td><td>90</td><td>0</td></tr><tr><td>103</td><td>Alexander</td><td>Hunold</td><td>AHUNOLD</td><td>590.423.4567</td><td>03-JAN-06</td><td>IT_PROG</td><td>9000</td><td> - </td><td>102</td><td>60</td><td>0</td></tr><tr><td>104</td><td>Bruce</td><td>Ernst</td><td>BERNST</td><td>590.423.4568</td><td>21-MAY-07</td><td>IT_PROG</td><td>6000</td><td> - </td><td>103</td><td>60</td><td>0</td></tr><tr><td>105</td><td>David</td><td>Austin</td><td>DAUSTIN</td><td>590.423.4569</td><td>25-JUN-05</td><td>IT_PROG</td><td>4800</td><td> - </td><td>103</td><td>60</td><td>0</td></tr><tr><td>106</td><td>Valli</td><td>Pataballa</td><td>VPATABAL</td><td>590.423.4560</td><td>05-FEB-06</td><td>IT_PROG</td><td>4800</td><td> - </td><td>103</td><td>60</td><td>0</td></tr><tr><td>107</td><td>Diana</td><td>Lorentz</td><td>DLORENTZ</td><td>590.423.5567</td><td>07-FEB-07</td><td>IT_PROG</td><td>4200</td><td> - </td><td>103</td><td>60</td><td>0</td></tr><tr><td>108</td><td>Nancy</td><td>Greenberg</td><td>NGREENBE</td><td>515.124.4569</td><td>17-AUG-02</td><td>FI_MGR</td><td>12008</td><td> - </td><td>101</td><td>100</td><td>0</td></tr><tr><td>109</td><td>Daniel</td><td>Faviet</td><td>DFAVIET</td><td>515.124.4169</td><td>16-AUG-02</td><td>FI_ACCOUNT</td><td>9000</td><td> - </td><td>108</td><td>100</td><td>0</td></tr><tr><td>110</td><td>John</td><td>Chen</td><td>JCHEN</td><td>515.124.4269</td><td>28-SEP-05</td><td>FI_ACCOUNT</td><td>8200</td><td> - </td><td>108</td><td>100</td><td>0</td></tr><tr><td>111</td><td>Ismael</td><td>Sciarra</td><td>ISCIARRA</td><td>515.124.4369</td><td>30-SEP-05</td><td>FI_ACCOUNT</td><td>7700</td><td> - </td><td>108</td><td>100</td><td>0</td></tr><tr><td>112</td><td>Jose Manuel</td><td>Urman</td><td>JMURMAN</td><td>515.124.4469</td><td>07-MAR-06</td><td>FI_ACCOUNT</td><td>7800</td><td> - </td><td>108</td><td>100</td><td>0</td></tr><tr><td>113</td><td>Luis</td><td>Popp</td><td>LPOPP</td><td>515.124.4567</td><td>07-DEC-07</td><td>FI_ACCOUNT</td><td>6900</td><td> - </td><td>108</td><td>100</td><td>0</td></tr><tr><td>114</td><td>Den</td><td>Raphaely</td><td>DRAPHEAL</td><td>515.127.4561</td><td>07-DEC-02</td><td>PU_MAN</td><td>11000</td><td> - </td><td>100</td><td>30</td><td>0</td></tr><tr><td>115</td><td>Alexander</td><td>Khoo</td><td>AKHOO</td><td>515.127.4562</td><td>18-MAY-03</td><td>PU_CLERK</td><td>3100</td><td> - </td><td>114</td><td>30</td><td>0</td></tr><tr><td>116</td><td>Shelli</td><td>Baida</td><td>SBAIDA</td><td>515.127.4563</td><td>24-DEC-05</td><td>PU_CLERK</td><td>2900</td><td> - </td><td>114</td><td>30</td><td>0</td></tr><tr><td>117</td><td>Sigal</td><td>Tobias</td><td>STOBIAS</td><td>515.127.4564</td><td>24-JUL-05</td><td>PU_CLERK</td><td>2800</td><td> - </td><td>114</td><td>30</td><td>0</td></tr><tr><td>118</td><td>Guy</td><td>Himuro</td><td>GHIMURO</td><td>515.127.4565</td><td>15-NOV-06</td><td>PU_CLERK</td><td>2600</td><td> - </td><td>114</td><td>30</td><td>0</td></tr><tr><td>119</td><td>Karen</td><td>Colmenares</td><td>KCOLMENA</td><td>515.127.4566</td><td>10-AUG-07</td><td>PU_CLERK</td><td>2500</td><td> - </td><td>114</td><td>30</td><td>0</td></tr><tr><td>120</td><td>Matthew</td><td>Weiss</td><td>MWEISS</td><td>650.123.1234</td><td>18-JUL-04</td><td>ST_MAN</td><td>8000</td><td> - </td><td>100</td><td>50</td><td>0</td></tr><tr><td>121</td><td>Adam</td><td>Fripp</td><td>AFRIPP</td><td>650.123.2234</td><td>10-APR-05</td><td>ST_MAN</td><td>8200</td><td> - </td><td>100</td><td>50</td><td>0</td></tr><tr><td>122</td><td>Payam</td><td>Kaufling</td><td>PKAUFLIN</td><td>650.123.3234</td><td>01-MAY-03</td><td>ST_MAN</td><td>7900</td><td> - </td><td>100</td><td>50</td><td>0</td></tr><tr><td>123</td><td>Shanta</td><td>Vollman</td><td>SVOLLMAN</td><td>650.123.4234</td><td>10-OCT-05</td><td>ST_MAN</td><td>6500</td><td> - </td><td>100</td><td>50</td><td>0</td></tr><tr><td>124</td><td>Kevin</td><td>Mourgos</td><td>KMOURGOS</td><td>650.123.5234</td><td>16-NOV-07</td><td>ST_MAN</td><td>5800</td><td> - </td><td>100</td><td>50</td><td>0</td></tr><tr><td>125</td><td>Julia</td><td>Nayer</td><td>JNAYER</td><td>650.124.1214</td><td>16-JUL-05</td><td>ST_CLERK</td><td>3200</td><td> - </td><td>120</td><td>50</td><td>0</td></tr><tr><td>126</td><td>Irene</td><td>Mikkilineni</td><td>IMIKKILI</td><td>650.124.1224</td><td>28-SEP-06</td><td>ST_CLERK</td><td>2700</td><td> - </td><td>120</td><td>50</td><td>0</td></tr><tr><td>127</td><td>James</td><td>Landry</td><td>JLANDRY</td><td>650.124.1334</td><td>14-JAN-07</td><td>ST_CLERK</td><td>2400</td><td> - </td><td>120</td><td>50</td><td>0</td></tr><tr><td>128</td><td>Steven</td><td>Markle</td><td>SMARKLE</td><td>650.124.1434</td><td>08-MAR-08</td><td>ST_CLERK</td><td>2200</td><td> - </td><td>120</td><td>50</td><td>0</td></tr><tr><td>129</td><td>Laura</td><td>Bissot</td><td>LBISSOT</td><td>650.124.5234</td><td>20-AUG-05</td><td>ST_CLERK</td><td>3300</td><td> - </td><td>121</td><td>50</td><td>0</td></tr><tr><td>130</td><td>Mozhe</td><td>Atkinson</td><td>MATKINSO</td><td>650.124.6234</td><td>30-OCT-05</td><td>ST_CLERK</td><td>2800</td><td> - </td><td>121</td><td>50</td><td>0</td></tr><tr><td>131</td><td>James</td><td>Marlow</td><td>JAMRLOW</td><td>650.124.7234</td><td>16-FEB-05</td><td>ST_CLERK</td><td>2500</td><td> - </td><td>121</td><td>50</td><td>0</td></tr><tr><td>132</td><td>TJ</td><td>Olson</td><td>TJOLSON</td><td>650.124.8234</td><td>10-APR-07</td><td>ST_CLERK</td><td>2100</td><td> - </td><td>121</td><td>50</td><td>0</td></tr><tr><td>133</td><td>Jason</td><td>Mallin</td><td>JMALLIN</td><td>650.127.1934</td><td>14-JUN-04</td><td>ST_CLERK</td><td>3300</td><td> - </td><td>122</td><td>50</td><td>0</td></tr><tr><td>134</td><td>Michael</td><td>Rogers</td><td>MROGERS</td><td>650.127.1834</td><td>26-AUG-06</td><td>ST_CLERK</td><td>2900</td><td> - </td><td>122</td><td>50</td><td>0</td></tr><tr><td>135</td><td>Ki</td><td>Gee</td><td>KGEE</td><td>650.127.1734</td><td>12-DEC-07</td><td>ST_CLERK</td><td>2400</td><td> - </td><td>122</td><td>50</td><td>0</td></tr><tr><td>136</td><td>Hazel</td><td>Philtanker</td><td>HPHILTAN</td><td>650.127.1634</td><td>06-FEB-08</td><td>ST_CLERK</td><td>2200</td><td> - </td><td>122</td><td>50</td><td>0</td></tr><tr><td>137</td><td>Renske</td><td>Ladwig</td><td>RLADWIG</td><td>650.121.1234</td><td>14-JUL-03</td><td>ST_CLERK</td><td>3600</td><td> - </td><td>123</td><td>50</td><td>0</td></tr><tr><td>138</td><td>Stephen</td><td>Stiles</td><td>SSTILES</td><td>650.121.2034</td><td>26-OCT-05</td><td>ST_CLERK</td><td>3200</td><td> - </td><td>123</td><td>50</td><td>0</td></tr><tr><td>139</td><td>John</td><td>Seo</td><td>JSEO</td><td>650.121.2019</td><td>12-FEB-06</td><td>ST_CLERK</td><td>2700</td><td> - </td><td>123</td><td>50</td><td>0</td></tr><tr><td>140</td><td>Joshua</td><td>Patel</td><td>JPATEL</td><td>650.121.1834</td><td>06-APR-06</td><td>ST_CLERK</td><td>2500</td><td> - </td><td>123</td><td>50</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         198,
         "Donald",
         "OConnell",
         "DOCONNEL",
         "650.507.9833",
         "21-JUN-07",
         "SH_CLERK",
         2600,
         " - ",
         "124",
         50,
         0
        ],
        [
         199,
         "Douglas",
         "Grant",
         "DGRANT",
         "650.507.9844",
         "13-JAN-08",
         "SH_CLERK",
         2600,
         " - ",
         "124",
         50,
         0
        ],
        [
         200,
         "Jennifer",
         "Whalen",
         "JWHALEN",
         "515.123.4444",
         "17-SEP-03",
         "AD_ASST",
         4400,
         " - ",
         "101",
         10,
         0
        ],
        [
         201,
         "Michael",
         "Hartstein",
         "MHARTSTE",
         "515.123.5555",
         "17-FEB-04",
         "MK_MAN",
         13000,
         " - ",
         "100",
         20,
         0
        ],
        [
         202,
         "Pat",
         "Fay",
         "PFAY",
         "603.123.6666",
         "17-AUG-05",
         "MK_REP",
         6000,
         " - ",
         "201",
         20,
         0
        ],
        [
         203,
         "Susan",
         "Mavris",
         "SMAVRIS",
         "515.123.7777",
         "07-JUN-02",
         "HR_REP",
         6500,
         " - ",
         "101",
         40,
         0
        ],
        [
         204,
         "Hermann",
         "Baer",
         "HBAER",
         "515.123.8888",
         "07-JUN-02",
         "PR_REP",
         10000,
         " - ",
         "101",
         70,
         0
        ],
        [
         205,
         "Shelley",
         "Higgins",
         "SHIGGINS",
         "515.123.8080",
         "07-JUN-02",
         "AC_MGR",
         12008,
         " - ",
         "101",
         110,
         0
        ],
        [
         206,
         "William",
         "Gietz",
         "WGIETZ",
         "515.123.8181",
         "07-JUN-02",
         "AC_ACCOUNT",
         8300,
         " - ",
         "205",
         110,
         0
        ],
        [
         100,
         "Steven",
         "King",
         "SKING",
         "515.123.4567",
         "17-JUN-03",
         "AD_PRES",
         24000,
         " - ",
         " - ",
         90,
         0
        ],
        [
         101,
         "Neena",
         "Kochhar",
         "NKOCHHAR",
         "515.123.4568",
         "21-SEP-05",
         "AD_VP",
         17000,
         " - ",
         "100",
         90,
         0
        ],
        [
         102,
         "Lex",
         "De Haan",
         "LDEHAAN",
         "515.123.4569",
         "13-JAN-01",
         "AD_VP",
         17000,
         " - ",
         "100",
         90,
         0
        ],
        [
         103,
         "Alexander",
         "Hunold",
         "AHUNOLD",
         "590.423.4567",
         "03-JAN-06",
         "IT_PROG",
         9000,
         " - ",
         "102",
         60,
         0
        ],
        [
         104,
         "Bruce",
         "Ernst",
         "BERNST",
         "590.423.4568",
         "21-MAY-07",
         "IT_PROG",
         6000,
         " - ",
         "103",
         60,
         0
        ],
        [
         105,
         "David",
         "Austin",
         "DAUSTIN",
         "590.423.4569",
         "25-JUN-05",
         "IT_PROG",
         4800,
         " - ",
         "103",
         60,
         0
        ],
        [
         106,
         "Valli",
         "Pataballa",
         "VPATABAL",
         "590.423.4560",
         "05-FEB-06",
         "IT_PROG",
         4800,
         " - ",
         "103",
         60,
         0
        ],
        [
         107,
         "Diana",
         "Lorentz",
         "DLORENTZ",
         "590.423.5567",
         "07-FEB-07",
         "IT_PROG",
         4200,
         " - ",
         "103",
         60,
         0
        ],
        [
         108,
         "Nancy",
         "Greenberg",
         "NGREENBE",
         "515.124.4569",
         "17-AUG-02",
         "FI_MGR",
         12008,
         " - ",
         "101",
         100,
         0
        ],
        [
         109,
         "Daniel",
         "Faviet",
         "DFAVIET",
         "515.124.4169",
         "16-AUG-02",
         "FI_ACCOUNT",
         9000,
         " - ",
         "108",
         100,
         0
        ],
        [
         110,
         "John",
         "Chen",
         "JCHEN",
         "515.124.4269",
         "28-SEP-05",
         "FI_ACCOUNT",
         8200,
         " - ",
         "108",
         100,
         0
        ],
        [
         111,
         "Ismael",
         "Sciarra",
         "ISCIARRA",
         "515.124.4369",
         "30-SEP-05",
         "FI_ACCOUNT",
         7700,
         " - ",
         "108",
         100,
         0
        ],
        [
         112,
         "Jose Manuel",
         "Urman",
         "JMURMAN",
         "515.124.4469",
         "07-MAR-06",
         "FI_ACCOUNT",
         7800,
         " - ",
         "108",
         100,
         0
        ],
        [
         113,
         "Luis",
         "Popp",
         "LPOPP",
         "515.124.4567",
         "07-DEC-07",
         "FI_ACCOUNT",
         6900,
         " - ",
         "108",
         100,
         0
        ],
        [
         114,
         "Den",
         "Raphaely",
         "DRAPHEAL",
         "515.127.4561",
         "07-DEC-02",
         "PU_MAN",
         11000,
         " - ",
         "100",
         30,
         0
        ],
        [
         115,
         "Alexander",
         "Khoo",
         "AKHOO",
         "515.127.4562",
         "18-MAY-03",
         "PU_CLERK",
         3100,
         " - ",
         "114",
         30,
         0
        ],
        [
         116,
         "Shelli",
         "Baida",
         "SBAIDA",
         "515.127.4563",
         "24-DEC-05",
         "PU_CLERK",
         2900,
         " - ",
         "114",
         30,
         0
        ],
        [
         117,
         "Sigal",
         "Tobias",
         "STOBIAS",
         "515.127.4564",
         "24-JUL-05",
         "PU_CLERK",
         2800,
         " - ",
         "114",
         30,
         0
        ],
        [
         118,
         "Guy",
         "Himuro",
         "GHIMURO",
         "515.127.4565",
         "15-NOV-06",
         "PU_CLERK",
         2600,
         " - ",
         "114",
         30,
         0
        ],
        [
         119,
         "Karen",
         "Colmenares",
         "KCOLMENA",
         "515.127.4566",
         "10-AUG-07",
         "PU_CLERK",
         2500,
         " - ",
         "114",
         30,
         0
        ],
        [
         120,
         "Matthew",
         "Weiss",
         "MWEISS",
         "650.123.1234",
         "18-JUL-04",
         "ST_MAN",
         8000,
         " - ",
         "100",
         50,
         0
        ],
        [
         121,
         "Adam",
         "Fripp",
         "AFRIPP",
         "650.123.2234",
         "10-APR-05",
         "ST_MAN",
         8200,
         " - ",
         "100",
         50,
         0
        ],
        [
         122,
         "Payam",
         "Kaufling",
         "PKAUFLIN",
         "650.123.3234",
         "01-MAY-03",
         "ST_MAN",
         7900,
         " - ",
         "100",
         50,
         0
        ],
        [
         123,
         "Shanta",
         "Vollman",
         "SVOLLMAN",
         "650.123.4234",
         "10-OCT-05",
         "ST_MAN",
         6500,
         " - ",
         "100",
         50,
         0
        ],
        [
         124,
         "Kevin",
         "Mourgos",
         "KMOURGOS",
         "650.123.5234",
         "16-NOV-07",
         "ST_MAN",
         5800,
         " - ",
         "100",
         50,
         0
        ],
        [
         125,
         "Julia",
         "Nayer",
         "JNAYER",
         "650.124.1214",
         "16-JUL-05",
         "ST_CLERK",
         3200,
         " - ",
         "120",
         50,
         0
        ],
        [
         126,
         "Irene",
         "Mikkilineni",
         "IMIKKILI",
         "650.124.1224",
         "28-SEP-06",
         "ST_CLERK",
         2700,
         " - ",
         "120",
         50,
         0
        ],
        [
         127,
         "James",
         "Landry",
         "JLANDRY",
         "650.124.1334",
         "14-JAN-07",
         "ST_CLERK",
         2400,
         " - ",
         "120",
         50,
         0
        ],
        [
         128,
         "Steven",
         "Markle",
         "SMARKLE",
         "650.124.1434",
         "08-MAR-08",
         "ST_CLERK",
         2200,
         " - ",
         "120",
         50,
         0
        ],
        [
         129,
         "Laura",
         "Bissot",
         "LBISSOT",
         "650.124.5234",
         "20-AUG-05",
         "ST_CLERK",
         3300,
         " - ",
         "121",
         50,
         0
        ],
        [
         130,
         "Mozhe",
         "Atkinson",
         "MATKINSO",
         "650.124.6234",
         "30-OCT-05",
         "ST_CLERK",
         2800,
         " - ",
         "121",
         50,
         0
        ],
        [
         131,
         "James",
         "Marlow",
         "JAMRLOW",
         "650.124.7234",
         "16-FEB-05",
         "ST_CLERK",
         2500,
         " - ",
         "121",
         50,
         0
        ],
        [
         132,
         "TJ",
         "Olson",
         "TJOLSON",
         "650.124.8234",
         "10-APR-07",
         "ST_CLERK",
         2100,
         " - ",
         "121",
         50,
         0
        ],
        [
         133,
         "Jason",
         "Mallin",
         "JMALLIN",
         "650.127.1934",
         "14-JUN-04",
         "ST_CLERK",
         3300,
         " - ",
         "122",
         50,
         0
        ],
        [
         134,
         "Michael",
         "Rogers",
         "MROGERS",
         "650.127.1834",
         "26-AUG-06",
         "ST_CLERK",
         2900,
         " - ",
         "122",
         50,
         0
        ],
        [
         135,
         "Ki",
         "Gee",
         "KGEE",
         "650.127.1734",
         "12-DEC-07",
         "ST_CLERK",
         2400,
         " - ",
         "122",
         50,
         0
        ],
        [
         136,
         "Hazel",
         "Philtanker",
         "HPHILTAN",
         "650.127.1634",
         "06-FEB-08",
         "ST_CLERK",
         2200,
         " - ",
         "122",
         50,
         0
        ],
        [
         137,
         "Renske",
         "Ladwig",
         "RLADWIG",
         "650.121.1234",
         "14-JUL-03",
         "ST_CLERK",
         3600,
         " - ",
         "123",
         50,
         0
        ],
        [
         138,
         "Stephen",
         "Stiles",
         "SSTILES",
         "650.121.2034",
         "26-OCT-05",
         "ST_CLERK",
         3200,
         " - ",
         "123",
         50,
         0
        ],
        [
         139,
         "John",
         "Seo",
         "JSEO",
         "650.121.2019",
         "12-FEB-06",
         "ST_CLERK",
         2700,
         " - ",
         "123",
         50,
         0
        ],
        [
         140,
         "Joshua",
         "Patel",
         "JPATEL",
         "650.121.1834",
         "06-APR-06",
         "ST_CLERK",
         2500,
         " - ",
         "123",
         50,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EMPLOYEE_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "FIRST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "LAST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EMAIL",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PHONE_NUMBER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HIRE_DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JOB_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SALARY",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "COMMISSION_PCT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "MANAGER_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEPARTMENT_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "pid",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_part_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f6e5f7-7909-4f5b-86d8-62a9c7e2f104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>pid</th><th>count</th></tr></thead><tbody><tr><td>0</td><td>50</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         50
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "pid",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_part_df.groupBy(\"pid\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c1fbd9-0009-4da6-87bb-d1c5e9d97534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atc_pre_departure_delays_2016.csv\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "\n",
    "ls /Volumes/sample_cat/default/volume_csv/sample_dir/atc_pre_departure_delays_2016.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1c9a54-532c-4bd3-8c99-6658769b65d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight_df = spark.read.option(\"header\",True)\\\n",
    "                        .option(\"inferSchema\",True)\\\n",
    "                        .csv(\"/Volumes/sample_cat/default/volume_csv/sample_dir/atc_pre_departure_delays_2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a4d058-058d-4bbb-a1ba-3f6170abbbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight_part_df = flight_df.withColumn(\"pid\",spark_partition_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ebe090-54fe-476c-8348-fd1c4f1b6aa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>pid</th><th>count</th></tr></thead><tbody><tr><td>0</td><td>17599</td></tr><tr><td>1</td><td>17540</td></tr><tr><td>2</td><td>17475</td></tr><tr><td>3</td><td>17465</td></tr><tr><td>4</td><td>17531</td></tr><tr><td>5</td><td>8987</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         17599
        ],
        [
         1,
         17540
        ],
        [
         2,
         17475
        ],
        [
         3,
         17465
        ],
        [
         4,
         17531
        ],
        [
         5,
         8987
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "pid",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flight_part_df.groupBy(\"pid\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4682d2d-b7a1-4c7a-909b-93172470fe3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight_part_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a878db-78e1-4762-906c-bc1bdd06a199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " flight_df_new = flight_df.coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c6912b-7b71-4513-9aef-0bbb345aa608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight_df_part_new = flight_df_new.withColumn(\"pid\",spark_partition_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0708079-127c-4cc1-8ceb-4f38af10cde4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "3 - parts\n",
    "\n",
    "100k\n",
    "100k\n",
    "10k\n",
    "\n",
    "3 tasks\n",
    "\n",
    "\n",
    "job 0\n",
    "-----\n",
    "task1 100k  --> will take more time to complete\n",
    "task2 100k  --> will take more time to complete\n",
    "task3 10k  --> will be completed very quickly \n",
    "\n",
    "job1\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6562896-ea6b-4063-8f35-bfe7a9520b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>pid</th><th>count</th></tr></thead><tbody><tr><td>0</td><td>35139</td></tr><tr><td>1</td><td>34940</td></tr><tr><td>2</td><td>26518</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         35139
        ],
        [
         1,
         34940
        ],
        [
         2,
         26518
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "pid",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flight_df_part_new.groupBy(\"pid\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "429d3e1f-d8bd-44ca-a878-f73b84d6f25a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# how to create temp view\n",
    "\n",
    "# createOrRepalceTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54387228-cc0f-44f4-8f8e-efab27cf67a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView(\"emp\")\n",
    "\n",
    "dept_df.createOrReplaceTempView('dept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8073e054-3c32-4c7c-962a-b5e8853bea14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEPARTMENT_ID</th><th>DEPARTMENT_NAME</th><th>MANAGER_ID</th><th>LOCATION_ID</th></tr></thead><tbody><tr><td>10</td><td>Administration</td><td>200</td><td>1700</td></tr><tr><td>20</td><td>Marketing</td><td>201</td><td>1800</td></tr><tr><td>30</td><td>Purchasing</td><td>114</td><td>1700</td></tr><tr><td>40</td><td>Human Resources</td><td>203</td><td>2400</td></tr><tr><td>50</td><td>Shipping</td><td>121</td><td>1500</td></tr><tr><td>60</td><td>IT</td><td>103</td><td>1400</td></tr><tr><td>70</td><td>Public Relations</td><td>204</td><td>2700</td></tr><tr><td>80</td><td>Sales</td><td>145</td><td>2500</td></tr><tr><td>90</td><td>Executive</td><td>100</td><td>1700</td></tr><tr><td>100</td><td>Finance</td><td>108</td><td>1700</td></tr><tr><td>110</td><td>Accounting</td><td>205</td><td>1700</td></tr><tr><td>120</td><td>Treasury</td><td>null</td><td>1700</td></tr><tr><td>130</td><td>Corporate Tax</td><td>null</td><td>1700</td></tr><tr><td>140</td><td>Control And Credit</td><td>null</td><td>1700</td></tr><tr><td>150</td><td>Shareholder Services</td><td>null</td><td>1700</td></tr><tr><td>160</td><td>Benefits</td><td>null</td><td>1700</td></tr><tr><td>170</td><td>Manufacturing</td><td>null</td><td>1700</td></tr><tr><td>180</td><td>Construction</td><td>null</td><td>1700</td></tr><tr><td>190</td><td>Contracting</td><td>null</td><td>1700</td></tr><tr><td>200</td><td>Operations</td><td>null</td><td>1700</td></tr><tr><td>210</td><td>IT Support</td><td>null</td><td>1700</td></tr><tr><td>220</td><td>NOC</td><td>null</td><td>1700</td></tr><tr><td>230</td><td>IT Helpdesk</td><td>null</td><td>1700</td></tr><tr><td>240</td><td>Government Sales</td><td>null</td><td>1700</td></tr><tr><td>250</td><td>Retail Sales</td><td>null</td><td>1700</td></tr><tr><td>260</td><td>Recruiting</td><td>null</td><td>1700</td></tr><tr><td>270</td><td>Payroll</td><td>null</td><td>1700</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Administration",
         200,
         1700
        ],
        [
         20,
         "Marketing",
         201,
         1800
        ],
        [
         30,
         "Purchasing",
         114,
         1700
        ],
        [
         40,
         "Human Resources",
         203,
         2400
        ],
        [
         50,
         "Shipping",
         121,
         1500
        ],
        [
         60,
         "IT",
         103,
         1400
        ],
        [
         70,
         "Public Relations",
         204,
         2700
        ],
        [
         80,
         "Sales",
         145,
         2500
        ],
        [
         90,
         "Executive",
         100,
         1700
        ],
        [
         100,
         "Finance",
         108,
         1700
        ],
        [
         110,
         "Accounting",
         205,
         1700
        ],
        [
         120,
         "Treasury",
         null,
         1700
        ],
        [
         130,
         "Corporate Tax",
         null,
         1700
        ],
        [
         140,
         "Control And Credit",
         null,
         1700
        ],
        [
         150,
         "Shareholder Services",
         null,
         1700
        ],
        [
         160,
         "Benefits",
         null,
         1700
        ],
        [
         170,
         "Manufacturing",
         null,
         1700
        ],
        [
         180,
         "Construction",
         null,
         1700
        ],
        [
         190,
         "Contracting",
         null,
         1700
        ],
        [
         200,
         "Operations",
         null,
         1700
        ],
        [
         210,
         "IT Support",
         null,
         1700
        ],
        [
         220,
         "NOC",
         null,
         1700
        ],
        [
         230,
         "IT Helpdesk",
         null,
         1700
        ],
        [
         240,
         "Government Sales",
         null,
         1700
        ],
        [
         250,
         "Retail Sales",
         null,
         1700
        ],
        [
         260,
         "Recruiting",
         null,
         1700
        ],
        [
         270,
         "Payroll",
         null,
         1700
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "DEPARTMENT_ID",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "DEPARTMENT_NAME",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "MANAGER_ID",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "LOCATION_ID",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 86
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEPARTMENT_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DEPARTMENT_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "MANAGER_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "LOCATION_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e46cca-4b6a-4592-9de9-08840c0aeea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EMPLOYEE_ID</th><th>FIRST_NAME</th><th>LAST_NAME</th><th>EMAIL</th><th>PHONE_NUMBER</th><th>HIRE_DATE</th><th>JOB_ID</th><th>SALARY</th><th>COMMISSION_PCT</th><th>MANAGER_ID</th><th>DEPARTMENT_ID</th></tr></thead><tbody><tr><td>198</td><td>Donald</td><td>OConnell</td><td>DOCONNEL</td><td>650.507.9833</td><td>21-JUN-07</td><td>SH_CLERK</td><td>2600</td><td> - </td><td>124</td><td>50</td></tr><tr><td>199</td><td>Douglas</td><td>Grant</td><td>DGRANT</td><td>650.507.9844</td><td>13-JAN-08</td><td>SH_CLERK</td><td>2600</td><td> - </td><td>124</td><td>50</td></tr><tr><td>200</td><td>Jennifer</td><td>Whalen</td><td>JWHALEN</td><td>515.123.4444</td><td>17-SEP-03</td><td>AD_ASST</td><td>4400</td><td> - </td><td>101</td><td>10</td></tr><tr><td>201</td><td>Michael</td><td>Hartstein</td><td>MHARTSTE</td><td>515.123.5555</td><td>17-FEB-04</td><td>MK_MAN</td><td>13000</td><td> - </td><td>100</td><td>20</td></tr><tr><td>202</td><td>Pat</td><td>Fay</td><td>PFAY</td><td>603.123.6666</td><td>17-AUG-05</td><td>MK_REP</td><td>6000</td><td> - </td><td>201</td><td>20</td></tr><tr><td>203</td><td>Susan</td><td>Mavris</td><td>SMAVRIS</td><td>515.123.7777</td><td>07-JUN-02</td><td>HR_REP</td><td>6500</td><td> - </td><td>101</td><td>40</td></tr><tr><td>204</td><td>Hermann</td><td>Baer</td><td>HBAER</td><td>515.123.8888</td><td>07-JUN-02</td><td>PR_REP</td><td>10000</td><td> - </td><td>101</td><td>70</td></tr><tr><td>205</td><td>Shelley</td><td>Higgins</td><td>SHIGGINS</td><td>515.123.8080</td><td>07-JUN-02</td><td>AC_MGR</td><td>12008</td><td> - </td><td>101</td><td>110</td></tr><tr><td>206</td><td>William</td><td>Gietz</td><td>WGIETZ</td><td>515.123.8181</td><td>07-JUN-02</td><td>AC_ACCOUNT</td><td>8300</td><td> - </td><td>205</td><td>110</td></tr><tr><td>100</td><td>Steven</td><td>King</td><td>SKING</td><td>515.123.4567</td><td>17-JUN-03</td><td>AD_PRES</td><td>24000</td><td> - </td><td> - </td><td>90</td></tr><tr><td>101</td><td>Neena</td><td>Kochhar</td><td>NKOCHHAR</td><td>515.123.4568</td><td>21-SEP-05</td><td>AD_VP</td><td>17000</td><td> - </td><td>100</td><td>90</td></tr><tr><td>102</td><td>Lex</td><td>De Haan</td><td>LDEHAAN</td><td>515.123.4569</td><td>13-JAN-01</td><td>AD_VP</td><td>17000</td><td> - </td><td>100</td><td>90</td></tr><tr><td>103</td><td>Alexander</td><td>Hunold</td><td>AHUNOLD</td><td>590.423.4567</td><td>03-JAN-06</td><td>IT_PROG</td><td>9000</td><td> - </td><td>102</td><td>60</td></tr><tr><td>104</td><td>Bruce</td><td>Ernst</td><td>BERNST</td><td>590.423.4568</td><td>21-MAY-07</td><td>IT_PROG</td><td>6000</td><td> - </td><td>103</td><td>60</td></tr><tr><td>105</td><td>David</td><td>Austin</td><td>DAUSTIN</td><td>590.423.4569</td><td>25-JUN-05</td><td>IT_PROG</td><td>4800</td><td> - </td><td>103</td><td>60</td></tr><tr><td>106</td><td>Valli</td><td>Pataballa</td><td>VPATABAL</td><td>590.423.4560</td><td>05-FEB-06</td><td>IT_PROG</td><td>4800</td><td> - </td><td>103</td><td>60</td></tr><tr><td>107</td><td>Diana</td><td>Lorentz</td><td>DLORENTZ</td><td>590.423.5567</td><td>07-FEB-07</td><td>IT_PROG</td><td>4200</td><td> - </td><td>103</td><td>60</td></tr><tr><td>108</td><td>Nancy</td><td>Greenberg</td><td>NGREENBE</td><td>515.124.4569</td><td>17-AUG-02</td><td>FI_MGR</td><td>12008</td><td> - </td><td>101</td><td>100</td></tr><tr><td>109</td><td>Daniel</td><td>Faviet</td><td>DFAVIET</td><td>515.124.4169</td><td>16-AUG-02</td><td>FI_ACCOUNT</td><td>9000</td><td> - </td><td>108</td><td>100</td></tr><tr><td>110</td><td>John</td><td>Chen</td><td>JCHEN</td><td>515.124.4269</td><td>28-SEP-05</td><td>FI_ACCOUNT</td><td>8200</td><td> - </td><td>108</td><td>100</td></tr><tr><td>111</td><td>Ismael</td><td>Sciarra</td><td>ISCIARRA</td><td>515.124.4369</td><td>30-SEP-05</td><td>FI_ACCOUNT</td><td>7700</td><td> - </td><td>108</td><td>100</td></tr><tr><td>112</td><td>Jose Manuel</td><td>Urman</td><td>JMURMAN</td><td>515.124.4469</td><td>07-MAR-06</td><td>FI_ACCOUNT</td><td>7800</td><td> - </td><td>108</td><td>100</td></tr><tr><td>113</td><td>Luis</td><td>Popp</td><td>LPOPP</td><td>515.124.4567</td><td>07-DEC-07</td><td>FI_ACCOUNT</td><td>6900</td><td> - </td><td>108</td><td>100</td></tr><tr><td>114</td><td>Den</td><td>Raphaely</td><td>DRAPHEAL</td><td>515.127.4561</td><td>07-DEC-02</td><td>PU_MAN</td><td>11000</td><td> - </td><td>100</td><td>30</td></tr><tr><td>115</td><td>Alexander</td><td>Khoo</td><td>AKHOO</td><td>515.127.4562</td><td>18-MAY-03</td><td>PU_CLERK</td><td>3100</td><td> - </td><td>114</td><td>30</td></tr><tr><td>116</td><td>Shelli</td><td>Baida</td><td>SBAIDA</td><td>515.127.4563</td><td>24-DEC-05</td><td>PU_CLERK</td><td>2900</td><td> - </td><td>114</td><td>30</td></tr><tr><td>117</td><td>Sigal</td><td>Tobias</td><td>STOBIAS</td><td>515.127.4564</td><td>24-JUL-05</td><td>PU_CLERK</td><td>2800</td><td> - </td><td>114</td><td>30</td></tr><tr><td>118</td><td>Guy</td><td>Himuro</td><td>GHIMURO</td><td>515.127.4565</td><td>15-NOV-06</td><td>PU_CLERK</td><td>2600</td><td> - </td><td>114</td><td>30</td></tr><tr><td>119</td><td>Karen</td><td>Colmenares</td><td>KCOLMENA</td><td>515.127.4566</td><td>10-AUG-07</td><td>PU_CLERK</td><td>2500</td><td> - </td><td>114</td><td>30</td></tr><tr><td>120</td><td>Matthew</td><td>Weiss</td><td>MWEISS</td><td>650.123.1234</td><td>18-JUL-04</td><td>ST_MAN</td><td>8000</td><td> - </td><td>100</td><td>50</td></tr><tr><td>121</td><td>Adam</td><td>Fripp</td><td>AFRIPP</td><td>650.123.2234</td><td>10-APR-05</td><td>ST_MAN</td><td>8200</td><td> - </td><td>100</td><td>50</td></tr><tr><td>122</td><td>Payam</td><td>Kaufling</td><td>PKAUFLIN</td><td>650.123.3234</td><td>01-MAY-03</td><td>ST_MAN</td><td>7900</td><td> - </td><td>100</td><td>50</td></tr><tr><td>123</td><td>Shanta</td><td>Vollman</td><td>SVOLLMAN</td><td>650.123.4234</td><td>10-OCT-05</td><td>ST_MAN</td><td>6500</td><td> - </td><td>100</td><td>50</td></tr><tr><td>124</td><td>Kevin</td><td>Mourgos</td><td>KMOURGOS</td><td>650.123.5234</td><td>16-NOV-07</td><td>ST_MAN</td><td>5800</td><td> - </td><td>100</td><td>50</td></tr><tr><td>125</td><td>Julia</td><td>Nayer</td><td>JNAYER</td><td>650.124.1214</td><td>16-JUL-05</td><td>ST_CLERK</td><td>3200</td><td> - </td><td>120</td><td>50</td></tr><tr><td>126</td><td>Irene</td><td>Mikkilineni</td><td>IMIKKILI</td><td>650.124.1224</td><td>28-SEP-06</td><td>ST_CLERK</td><td>2700</td><td> - </td><td>120</td><td>50</td></tr><tr><td>127</td><td>James</td><td>Landry</td><td>JLANDRY</td><td>650.124.1334</td><td>14-JAN-07</td><td>ST_CLERK</td><td>2400</td><td> - </td><td>120</td><td>50</td></tr><tr><td>128</td><td>Steven</td><td>Markle</td><td>SMARKLE</td><td>650.124.1434</td><td>08-MAR-08</td><td>ST_CLERK</td><td>2200</td><td> - </td><td>120</td><td>50</td></tr><tr><td>129</td><td>Laura</td><td>Bissot</td><td>LBISSOT</td><td>650.124.5234</td><td>20-AUG-05</td><td>ST_CLERK</td><td>3300</td><td> - </td><td>121</td><td>50</td></tr><tr><td>130</td><td>Mozhe</td><td>Atkinson</td><td>MATKINSO</td><td>650.124.6234</td><td>30-OCT-05</td><td>ST_CLERK</td><td>2800</td><td> - </td><td>121</td><td>50</td></tr><tr><td>131</td><td>James</td><td>Marlow</td><td>JAMRLOW</td><td>650.124.7234</td><td>16-FEB-05</td><td>ST_CLERK</td><td>2500</td><td> - </td><td>121</td><td>50</td></tr><tr><td>132</td><td>TJ</td><td>Olson</td><td>TJOLSON</td><td>650.124.8234</td><td>10-APR-07</td><td>ST_CLERK</td><td>2100</td><td> - </td><td>121</td><td>50</td></tr><tr><td>133</td><td>Jason</td><td>Mallin</td><td>JMALLIN</td><td>650.127.1934</td><td>14-JUN-04</td><td>ST_CLERK</td><td>3300</td><td> - </td><td>122</td><td>50</td></tr><tr><td>134</td><td>Michael</td><td>Rogers</td><td>MROGERS</td><td>650.127.1834</td><td>26-AUG-06</td><td>ST_CLERK</td><td>2900</td><td> - </td><td>122</td><td>50</td></tr><tr><td>135</td><td>Ki</td><td>Gee</td><td>KGEE</td><td>650.127.1734</td><td>12-DEC-07</td><td>ST_CLERK</td><td>2400</td><td> - </td><td>122</td><td>50</td></tr><tr><td>136</td><td>Hazel</td><td>Philtanker</td><td>HPHILTAN</td><td>650.127.1634</td><td>06-FEB-08</td><td>ST_CLERK</td><td>2200</td><td> - </td><td>122</td><td>50</td></tr><tr><td>137</td><td>Renske</td><td>Ladwig</td><td>RLADWIG</td><td>650.121.1234</td><td>14-JUL-03</td><td>ST_CLERK</td><td>3600</td><td> - </td><td>123</td><td>50</td></tr><tr><td>138</td><td>Stephen</td><td>Stiles</td><td>SSTILES</td><td>650.121.2034</td><td>26-OCT-05</td><td>ST_CLERK</td><td>3200</td><td> - </td><td>123</td><td>50</td></tr><tr><td>139</td><td>John</td><td>Seo</td><td>JSEO</td><td>650.121.2019</td><td>12-FEB-06</td><td>ST_CLERK</td><td>2700</td><td> - </td><td>123</td><td>50</td></tr><tr><td>140</td><td>Joshua</td><td>Patel</td><td>JPATEL</td><td>650.121.1834</td><td>06-APR-06</td><td>ST_CLERK</td><td>2500</td><td> - </td><td>123</td><td>50</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         198,
         "Donald",
         "OConnell",
         "DOCONNEL",
         "650.507.9833",
         "21-JUN-07",
         "SH_CLERK",
         2600,
         " - ",
         "124",
         50
        ],
        [
         199,
         "Douglas",
         "Grant",
         "DGRANT",
         "650.507.9844",
         "13-JAN-08",
         "SH_CLERK",
         2600,
         " - ",
         "124",
         50
        ],
        [
         200,
         "Jennifer",
         "Whalen",
         "JWHALEN",
         "515.123.4444",
         "17-SEP-03",
         "AD_ASST",
         4400,
         " - ",
         "101",
         10
        ],
        [
         201,
         "Michael",
         "Hartstein",
         "MHARTSTE",
         "515.123.5555",
         "17-FEB-04",
         "MK_MAN",
         13000,
         " - ",
         "100",
         20
        ],
        [
         202,
         "Pat",
         "Fay",
         "PFAY",
         "603.123.6666",
         "17-AUG-05",
         "MK_REP",
         6000,
         " - ",
         "201",
         20
        ],
        [
         203,
         "Susan",
         "Mavris",
         "SMAVRIS",
         "515.123.7777",
         "07-JUN-02",
         "HR_REP",
         6500,
         " - ",
         "101",
         40
        ],
        [
         204,
         "Hermann",
         "Baer",
         "HBAER",
         "515.123.8888",
         "07-JUN-02",
         "PR_REP",
         10000,
         " - ",
         "101",
         70
        ],
        [
         205,
         "Shelley",
         "Higgins",
         "SHIGGINS",
         "515.123.8080",
         "07-JUN-02",
         "AC_MGR",
         12008,
         " - ",
         "101",
         110
        ],
        [
         206,
         "William",
         "Gietz",
         "WGIETZ",
         "515.123.8181",
         "07-JUN-02",
         "AC_ACCOUNT",
         8300,
         " - ",
         "205",
         110
        ],
        [
         100,
         "Steven",
         "King",
         "SKING",
         "515.123.4567",
         "17-JUN-03",
         "AD_PRES",
         24000,
         " - ",
         " - ",
         90
        ],
        [
         101,
         "Neena",
         "Kochhar",
         "NKOCHHAR",
         "515.123.4568",
         "21-SEP-05",
         "AD_VP",
         17000,
         " - ",
         "100",
         90
        ],
        [
         102,
         "Lex",
         "De Haan",
         "LDEHAAN",
         "515.123.4569",
         "13-JAN-01",
         "AD_VP",
         17000,
         " - ",
         "100",
         90
        ],
        [
         103,
         "Alexander",
         "Hunold",
         "AHUNOLD",
         "590.423.4567",
         "03-JAN-06",
         "IT_PROG",
         9000,
         " - ",
         "102",
         60
        ],
        [
         104,
         "Bruce",
         "Ernst",
         "BERNST",
         "590.423.4568",
         "21-MAY-07",
         "IT_PROG",
         6000,
         " - ",
         "103",
         60
        ],
        [
         105,
         "David",
         "Austin",
         "DAUSTIN",
         "590.423.4569",
         "25-JUN-05",
         "IT_PROG",
         4800,
         " - ",
         "103",
         60
        ],
        [
         106,
         "Valli",
         "Pataballa",
         "VPATABAL",
         "590.423.4560",
         "05-FEB-06",
         "IT_PROG",
         4800,
         " - ",
         "103",
         60
        ],
        [
         107,
         "Diana",
         "Lorentz",
         "DLORENTZ",
         "590.423.5567",
         "07-FEB-07",
         "IT_PROG",
         4200,
         " - ",
         "103",
         60
        ],
        [
         108,
         "Nancy",
         "Greenberg",
         "NGREENBE",
         "515.124.4569",
         "17-AUG-02",
         "FI_MGR",
         12008,
         " - ",
         "101",
         100
        ],
        [
         109,
         "Daniel",
         "Faviet",
         "DFAVIET",
         "515.124.4169",
         "16-AUG-02",
         "FI_ACCOUNT",
         9000,
         " - ",
         "108",
         100
        ],
        [
         110,
         "John",
         "Chen",
         "JCHEN",
         "515.124.4269",
         "28-SEP-05",
         "FI_ACCOUNT",
         8200,
         " - ",
         "108",
         100
        ],
        [
         111,
         "Ismael",
         "Sciarra",
         "ISCIARRA",
         "515.124.4369",
         "30-SEP-05",
         "FI_ACCOUNT",
         7700,
         " - ",
         "108",
         100
        ],
        [
         112,
         "Jose Manuel",
         "Urman",
         "JMURMAN",
         "515.124.4469",
         "07-MAR-06",
         "FI_ACCOUNT",
         7800,
         " - ",
         "108",
         100
        ],
        [
         113,
         "Luis",
         "Popp",
         "LPOPP",
         "515.124.4567",
         "07-DEC-07",
         "FI_ACCOUNT",
         6900,
         " - ",
         "108",
         100
        ],
        [
         114,
         "Den",
         "Raphaely",
         "DRAPHEAL",
         "515.127.4561",
         "07-DEC-02",
         "PU_MAN",
         11000,
         " - ",
         "100",
         30
        ],
        [
         115,
         "Alexander",
         "Khoo",
         "AKHOO",
         "515.127.4562",
         "18-MAY-03",
         "PU_CLERK",
         3100,
         " - ",
         "114",
         30
        ],
        [
         116,
         "Shelli",
         "Baida",
         "SBAIDA",
         "515.127.4563",
         "24-DEC-05",
         "PU_CLERK",
         2900,
         " - ",
         "114",
         30
        ],
        [
         117,
         "Sigal",
         "Tobias",
         "STOBIAS",
         "515.127.4564",
         "24-JUL-05",
         "PU_CLERK",
         2800,
         " - ",
         "114",
         30
        ],
        [
         118,
         "Guy",
         "Himuro",
         "GHIMURO",
         "515.127.4565",
         "15-NOV-06",
         "PU_CLERK",
         2600,
         " - ",
         "114",
         30
        ],
        [
         119,
         "Karen",
         "Colmenares",
         "KCOLMENA",
         "515.127.4566",
         "10-AUG-07",
         "PU_CLERK",
         2500,
         " - ",
         "114",
         30
        ],
        [
         120,
         "Matthew",
         "Weiss",
         "MWEISS",
         "650.123.1234",
         "18-JUL-04",
         "ST_MAN",
         8000,
         " - ",
         "100",
         50
        ],
        [
         121,
         "Adam",
         "Fripp",
         "AFRIPP",
         "650.123.2234",
         "10-APR-05",
         "ST_MAN",
         8200,
         " - ",
         "100",
         50
        ],
        [
         122,
         "Payam",
         "Kaufling",
         "PKAUFLIN",
         "650.123.3234",
         "01-MAY-03",
         "ST_MAN",
         7900,
         " - ",
         "100",
         50
        ],
        [
         123,
         "Shanta",
         "Vollman",
         "SVOLLMAN",
         "650.123.4234",
         "10-OCT-05",
         "ST_MAN",
         6500,
         " - ",
         "100",
         50
        ],
        [
         124,
         "Kevin",
         "Mourgos",
         "KMOURGOS",
         "650.123.5234",
         "16-NOV-07",
         "ST_MAN",
         5800,
         " - ",
         "100",
         50
        ],
        [
         125,
         "Julia",
         "Nayer",
         "JNAYER",
         "650.124.1214",
         "16-JUL-05",
         "ST_CLERK",
         3200,
         " - ",
         "120",
         50
        ],
        [
         126,
         "Irene",
         "Mikkilineni",
         "IMIKKILI",
         "650.124.1224",
         "28-SEP-06",
         "ST_CLERK",
         2700,
         " - ",
         "120",
         50
        ],
        [
         127,
         "James",
         "Landry",
         "JLANDRY",
         "650.124.1334",
         "14-JAN-07",
         "ST_CLERK",
         2400,
         " - ",
         "120",
         50
        ],
        [
         128,
         "Steven",
         "Markle",
         "SMARKLE",
         "650.124.1434",
         "08-MAR-08",
         "ST_CLERK",
         2200,
         " - ",
         "120",
         50
        ],
        [
         129,
         "Laura",
         "Bissot",
         "LBISSOT",
         "650.124.5234",
         "20-AUG-05",
         "ST_CLERK",
         3300,
         " - ",
         "121",
         50
        ],
        [
         130,
         "Mozhe",
         "Atkinson",
         "MATKINSO",
         "650.124.6234",
         "30-OCT-05",
         "ST_CLERK",
         2800,
         " - ",
         "121",
         50
        ],
        [
         131,
         "James",
         "Marlow",
         "JAMRLOW",
         "650.124.7234",
         "16-FEB-05",
         "ST_CLERK",
         2500,
         " - ",
         "121",
         50
        ],
        [
         132,
         "TJ",
         "Olson",
         "TJOLSON",
         "650.124.8234",
         "10-APR-07",
         "ST_CLERK",
         2100,
         " - ",
         "121",
         50
        ],
        [
         133,
         "Jason",
         "Mallin",
         "JMALLIN",
         "650.127.1934",
         "14-JUN-04",
         "ST_CLERK",
         3300,
         " - ",
         "122",
         50
        ],
        [
         134,
         "Michael",
         "Rogers",
         "MROGERS",
         "650.127.1834",
         "26-AUG-06",
         "ST_CLERK",
         2900,
         " - ",
         "122",
         50
        ],
        [
         135,
         "Ki",
         "Gee",
         "KGEE",
         "650.127.1734",
         "12-DEC-07",
         "ST_CLERK",
         2400,
         " - ",
         "122",
         50
        ],
        [
         136,
         "Hazel",
         "Philtanker",
         "HPHILTAN",
         "650.127.1634",
         "06-FEB-08",
         "ST_CLERK",
         2200,
         " - ",
         "122",
         50
        ],
        [
         137,
         "Renske",
         "Ladwig",
         "RLADWIG",
         "650.121.1234",
         "14-JUL-03",
         "ST_CLERK",
         3600,
         " - ",
         "123",
         50
        ],
        [
         138,
         "Stephen",
         "Stiles",
         "SSTILES",
         "650.121.2034",
         "26-OCT-05",
         "ST_CLERK",
         3200,
         " - ",
         "123",
         50
        ],
        [
         139,
         "John",
         "Seo",
         "JSEO",
         "650.121.2019",
         "12-FEB-06",
         "ST_CLERK",
         2700,
         " - ",
         "123",
         50
        ],
        [
         140,
         "Joshua",
         "Patel",
         "JPATEL",
         "650.121.1834",
         "06-APR-06",
         "ST_CLERK",
         2500,
         " - ",
         "123",
         50
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "EMPLOYEE_ID",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "FIRST_NAME",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "LAST_NAME",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "EMAIL",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "PHONE_NUMBER",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "HIRE_DATE",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "JOB_ID",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "SALARY",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "COMMISSION_PCT",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "MANAGER_ID",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "DEPARTMENT_ID",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 85
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EMPLOYEE_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "FIRST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "LAST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EMAIL",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PHONE_NUMBER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HIRE_DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JOB_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SALARY",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "COMMISSION_PCT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "MANAGER_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEPARTMENT_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from emp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11c9c6a5-a1be-47c5-989f-2e094ed9597f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# i want to get emp_id, fist_name, deptname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a762a61-aa87-44b0-b953-3ec619d6eca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EMPLOYEE_ID</th><th>FIRST_NAME</th><th>DEPARTMENT_NAME</th></tr></thead><tbody><tr><td>198</td><td>Donald</td><td>Shipping</td></tr><tr><td>199</td><td>Douglas</td><td>Shipping</td></tr><tr><td>200</td><td>Jennifer</td><td>Administration</td></tr><tr><td>201</td><td>Michael</td><td>Marketing</td></tr><tr><td>202</td><td>Pat</td><td>Marketing</td></tr><tr><td>203</td><td>Susan</td><td>Human Resources</td></tr><tr><td>204</td><td>Hermann</td><td>Public Relations</td></tr><tr><td>205</td><td>Shelley</td><td>Accounting</td></tr><tr><td>206</td><td>William</td><td>Accounting</td></tr><tr><td>100</td><td>Steven</td><td>Executive</td></tr><tr><td>101</td><td>Neena</td><td>Executive</td></tr><tr><td>102</td><td>Lex</td><td>Executive</td></tr><tr><td>103</td><td>Alexander</td><td>IT</td></tr><tr><td>104</td><td>Bruce</td><td>IT</td></tr><tr><td>105</td><td>David</td><td>IT</td></tr><tr><td>106</td><td>Valli</td><td>IT</td></tr><tr><td>107</td><td>Diana</td><td>IT</td></tr><tr><td>108</td><td>Nancy</td><td>Finance</td></tr><tr><td>109</td><td>Daniel</td><td>Finance</td></tr><tr><td>110</td><td>John</td><td>Finance</td></tr><tr><td>111</td><td>Ismael</td><td>Finance</td></tr><tr><td>112</td><td>Jose Manuel</td><td>Finance</td></tr><tr><td>113</td><td>Luis</td><td>Finance</td></tr><tr><td>114</td><td>Den</td><td>Purchasing</td></tr><tr><td>115</td><td>Alexander</td><td>Purchasing</td></tr><tr><td>116</td><td>Shelli</td><td>Purchasing</td></tr><tr><td>117</td><td>Sigal</td><td>Purchasing</td></tr><tr><td>118</td><td>Guy</td><td>Purchasing</td></tr><tr><td>119</td><td>Karen</td><td>Purchasing</td></tr><tr><td>120</td><td>Matthew</td><td>Shipping</td></tr><tr><td>121</td><td>Adam</td><td>Shipping</td></tr><tr><td>122</td><td>Payam</td><td>Shipping</td></tr><tr><td>123</td><td>Shanta</td><td>Shipping</td></tr><tr><td>124</td><td>Kevin</td><td>Shipping</td></tr><tr><td>125</td><td>Julia</td><td>Shipping</td></tr><tr><td>126</td><td>Irene</td><td>Shipping</td></tr><tr><td>127</td><td>James</td><td>Shipping</td></tr><tr><td>128</td><td>Steven</td><td>Shipping</td></tr><tr><td>129</td><td>Laura</td><td>Shipping</td></tr><tr><td>130</td><td>Mozhe</td><td>Shipping</td></tr><tr><td>131</td><td>James</td><td>Shipping</td></tr><tr><td>132</td><td>TJ</td><td>Shipping</td></tr><tr><td>133</td><td>Jason</td><td>Shipping</td></tr><tr><td>134</td><td>Michael</td><td>Shipping</td></tr><tr><td>135</td><td>Ki</td><td>Shipping</td></tr><tr><td>136</td><td>Hazel</td><td>Shipping</td></tr><tr><td>137</td><td>Renske</td><td>Shipping</td></tr><tr><td>138</td><td>Stephen</td><td>Shipping</td></tr><tr><td>139</td><td>John</td><td>Shipping</td></tr><tr><td>140</td><td>Joshua</td><td>Shipping</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         198,
         "Donald",
         "Shipping"
        ],
        [
         199,
         "Douglas",
         "Shipping"
        ],
        [
         200,
         "Jennifer",
         "Administration"
        ],
        [
         201,
         "Michael",
         "Marketing"
        ],
        [
         202,
         "Pat",
         "Marketing"
        ],
        [
         203,
         "Susan",
         "Human Resources"
        ],
        [
         204,
         "Hermann",
         "Public Relations"
        ],
        [
         205,
         "Shelley",
         "Accounting"
        ],
        [
         206,
         "William",
         "Accounting"
        ],
        [
         100,
         "Steven",
         "Executive"
        ],
        [
         101,
         "Neena",
         "Executive"
        ],
        [
         102,
         "Lex",
         "Executive"
        ],
        [
         103,
         "Alexander",
         "IT"
        ],
        [
         104,
         "Bruce",
         "IT"
        ],
        [
         105,
         "David",
         "IT"
        ],
        [
         106,
         "Valli",
         "IT"
        ],
        [
         107,
         "Diana",
         "IT"
        ],
        [
         108,
         "Nancy",
         "Finance"
        ],
        [
         109,
         "Daniel",
         "Finance"
        ],
        [
         110,
         "John",
         "Finance"
        ],
        [
         111,
         "Ismael",
         "Finance"
        ],
        [
         112,
         "Jose Manuel",
         "Finance"
        ],
        [
         113,
         "Luis",
         "Finance"
        ],
        [
         114,
         "Den",
         "Purchasing"
        ],
        [
         115,
         "Alexander",
         "Purchasing"
        ],
        [
         116,
         "Shelli",
         "Purchasing"
        ],
        [
         117,
         "Sigal",
         "Purchasing"
        ],
        [
         118,
         "Guy",
         "Purchasing"
        ],
        [
         119,
         "Karen",
         "Purchasing"
        ],
        [
         120,
         "Matthew",
         "Shipping"
        ],
        [
         121,
         "Adam",
         "Shipping"
        ],
        [
         122,
         "Payam",
         "Shipping"
        ],
        [
         123,
         "Shanta",
         "Shipping"
        ],
        [
         124,
         "Kevin",
         "Shipping"
        ],
        [
         125,
         "Julia",
         "Shipping"
        ],
        [
         126,
         "Irene",
         "Shipping"
        ],
        [
         127,
         "James",
         "Shipping"
        ],
        [
         128,
         "Steven",
         "Shipping"
        ],
        [
         129,
         "Laura",
         "Shipping"
        ],
        [
         130,
         "Mozhe",
         "Shipping"
        ],
        [
         131,
         "James",
         "Shipping"
        ],
        [
         132,
         "TJ",
         "Shipping"
        ],
        [
         133,
         "Jason",
         "Shipping"
        ],
        [
         134,
         "Michael",
         "Shipping"
        ],
        [
         135,
         "Ki",
         "Shipping"
        ],
        [
         136,
         "Hazel",
         "Shipping"
        ],
        [
         137,
         "Renske",
         "Shipping"
        ],
        [
         138,
         "Stephen",
         "Shipping"
        ],
        [
         139,
         "John",
         "Shipping"
        ],
        [
         140,
         "Joshua",
         "Shipping"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "EMPLOYEE_ID",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "FIRST_NAME",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "DEPARTMENT_NAME",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 91
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EMPLOYEE_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "FIRST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEPARTMENT_NAME",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "select e.EMPLOYEE_ID, e.FIRST_NAME, d.DEPARTMENT_NAME from emp e join dept d on e.DEPARTMENT_ID = d.DEPARTMENT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f57095a-f364-4183-be4a-cc6097cd8243",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_joined_df = spark.sql(\"select e.EMPLOYEE_ID, e.FIRST_NAME, d.DEPARTMENT_NAME from emp e join dept d on e.DEPARTMENT_ID = d.DEPARTMENT_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8850ec0-3643-4918-ad17-ce13b93bdea2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761056999790}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EMPLOYEE_ID</th><th>FIRST_NAME</th><th>DEPARTMENT_NAME</th></tr></thead><tbody><tr><td>198</td><td>Donald</td><td>Shipping</td></tr><tr><td>199</td><td>Douglas</td><td>Shipping</td></tr><tr><td>200</td><td>Jennifer</td><td>Administration</td></tr><tr><td>201</td><td>Michael</td><td>Marketing</td></tr><tr><td>202</td><td>Pat</td><td>Marketing</td></tr><tr><td>203</td><td>Susan</td><td>Human Resources</td></tr><tr><td>204</td><td>Hermann</td><td>Public Relations</td></tr><tr><td>205</td><td>Shelley</td><td>Accounting</td></tr><tr><td>206</td><td>William</td><td>Accounting</td></tr><tr><td>100</td><td>Steven</td><td>Executive</td></tr><tr><td>101</td><td>Neena</td><td>Executive</td></tr><tr><td>102</td><td>Lex</td><td>Executive</td></tr><tr><td>103</td><td>Alexander</td><td>IT</td></tr><tr><td>104</td><td>Bruce</td><td>IT</td></tr><tr><td>105</td><td>David</td><td>IT</td></tr><tr><td>106</td><td>Valli</td><td>IT</td></tr><tr><td>107</td><td>Diana</td><td>IT</td></tr><tr><td>108</td><td>Nancy</td><td>Finance</td></tr><tr><td>109</td><td>Daniel</td><td>Finance</td></tr><tr><td>110</td><td>John</td><td>Finance</td></tr><tr><td>111</td><td>Ismael</td><td>Finance</td></tr><tr><td>112</td><td>Jose Manuel</td><td>Finance</td></tr><tr><td>113</td><td>Luis</td><td>Finance</td></tr><tr><td>114</td><td>Den</td><td>Purchasing</td></tr><tr><td>115</td><td>Alexander</td><td>Purchasing</td></tr><tr><td>116</td><td>Shelli</td><td>Purchasing</td></tr><tr><td>117</td><td>Sigal</td><td>Purchasing</td></tr><tr><td>118</td><td>Guy</td><td>Purchasing</td></tr><tr><td>119</td><td>Karen</td><td>Purchasing</td></tr><tr><td>120</td><td>Matthew</td><td>Shipping</td></tr><tr><td>121</td><td>Adam</td><td>Shipping</td></tr><tr><td>122</td><td>Payam</td><td>Shipping</td></tr><tr><td>123</td><td>Shanta</td><td>Shipping</td></tr><tr><td>124</td><td>Kevin</td><td>Shipping</td></tr><tr><td>125</td><td>Julia</td><td>Shipping</td></tr><tr><td>126</td><td>Irene</td><td>Shipping</td></tr><tr><td>127</td><td>James</td><td>Shipping</td></tr><tr><td>128</td><td>Steven</td><td>Shipping</td></tr><tr><td>129</td><td>Laura</td><td>Shipping</td></tr><tr><td>130</td><td>Mozhe</td><td>Shipping</td></tr><tr><td>131</td><td>James</td><td>Shipping</td></tr><tr><td>132</td><td>TJ</td><td>Shipping</td></tr><tr><td>133</td><td>Jason</td><td>Shipping</td></tr><tr><td>134</td><td>Michael</td><td>Shipping</td></tr><tr><td>135</td><td>Ki</td><td>Shipping</td></tr><tr><td>136</td><td>Hazel</td><td>Shipping</td></tr><tr><td>137</td><td>Renske</td><td>Shipping</td></tr><tr><td>138</td><td>Stephen</td><td>Shipping</td></tr><tr><td>139</td><td>John</td><td>Shipping</td></tr><tr><td>140</td><td>Joshua</td><td>Shipping</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         198,
         "Donald",
         "Shipping"
        ],
        [
         199,
         "Douglas",
         "Shipping"
        ],
        [
         200,
         "Jennifer",
         "Administration"
        ],
        [
         201,
         "Michael",
         "Marketing"
        ],
        [
         202,
         "Pat",
         "Marketing"
        ],
        [
         203,
         "Susan",
         "Human Resources"
        ],
        [
         204,
         "Hermann",
         "Public Relations"
        ],
        [
         205,
         "Shelley",
         "Accounting"
        ],
        [
         206,
         "William",
         "Accounting"
        ],
        [
         100,
         "Steven",
         "Executive"
        ],
        [
         101,
         "Neena",
         "Executive"
        ],
        [
         102,
         "Lex",
         "Executive"
        ],
        [
         103,
         "Alexander",
         "IT"
        ],
        [
         104,
         "Bruce",
         "IT"
        ],
        [
         105,
         "David",
         "IT"
        ],
        [
         106,
         "Valli",
         "IT"
        ],
        [
         107,
         "Diana",
         "IT"
        ],
        [
         108,
         "Nancy",
         "Finance"
        ],
        [
         109,
         "Daniel",
         "Finance"
        ],
        [
         110,
         "John",
         "Finance"
        ],
        [
         111,
         "Ismael",
         "Finance"
        ],
        [
         112,
         "Jose Manuel",
         "Finance"
        ],
        [
         113,
         "Luis",
         "Finance"
        ],
        [
         114,
         "Den",
         "Purchasing"
        ],
        [
         115,
         "Alexander",
         "Purchasing"
        ],
        [
         116,
         "Shelli",
         "Purchasing"
        ],
        [
         117,
         "Sigal",
         "Purchasing"
        ],
        [
         118,
         "Guy",
         "Purchasing"
        ],
        [
         119,
         "Karen",
         "Purchasing"
        ],
        [
         120,
         "Matthew",
         "Shipping"
        ],
        [
         121,
         "Adam",
         "Shipping"
        ],
        [
         122,
         "Payam",
         "Shipping"
        ],
        [
         123,
         "Shanta",
         "Shipping"
        ],
        [
         124,
         "Kevin",
         "Shipping"
        ],
        [
         125,
         "Julia",
         "Shipping"
        ],
        [
         126,
         "Irene",
         "Shipping"
        ],
        [
         127,
         "James",
         "Shipping"
        ],
        [
         128,
         "Steven",
         "Shipping"
        ],
        [
         129,
         "Laura",
         "Shipping"
        ],
        [
         130,
         "Mozhe",
         "Shipping"
        ],
        [
         131,
         "James",
         "Shipping"
        ],
        [
         132,
         "TJ",
         "Shipping"
        ],
        [
         133,
         "Jason",
         "Shipping"
        ],
        [
         134,
         "Michael",
         "Shipping"
        ],
        [
         135,
         "Ki",
         "Shipping"
        ],
        [
         136,
         "Hazel",
         "Shipping"
        ],
        [
         137,
         "Renske",
         "Shipping"
        ],
        [
         138,
         "Stephen",
         "Shipping"
        ],
        [
         139,
         "John",
         "Shipping"
        ],
        [
         140,
         "Joshua",
         "Shipping"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EMPLOYEE_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "FIRST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEPARTMENT_NAME",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_joined_df.display()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a7efa5-cc71-4698-a97c-a18e88ec339d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# task\n",
    "\n",
    "emp, dept \n",
    "\n",
    "read to datasets \n",
    "\n",
    "emp_full_name_list  dept_id  dept_name\n",
    "Shelley|William      10       Accounting\n",
    "\n",
    "\n",
    "write down it to a table -- create a table \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6040872030332679,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_session4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}